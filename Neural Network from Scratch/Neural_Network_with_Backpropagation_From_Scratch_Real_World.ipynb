{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from random import randrange\n",
    "from random import random\n",
    "from csv import reader\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['15.26', '14.84', '0.871', '5.763', '3.312', '2.221', '5.22', '1'],\n",
       " ['14.88', '14.57', '0.8811', '5.554', '3.333', '1.018', '4.956', '1'],\n",
       " ['14.29', '14.09', '0.905', '5.291', '3.337', '2.699', '4.825', '1'],\n",
       " ['13.84', '13.94', '0.8955', '5.324', '3.379', '2.259', '4.805', '1'],\n",
       " ['16.14', '14.99', '0.9034', '5.658', '3.562', '1.355', '5.175', '1']]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'wheat-seeds.csv'\n",
    "dataset = load_csv(filename)\n",
    "#Print 5 first rows\n",
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "#Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[15.26, 14.84, 0.871, 5.763, 3.312, 2.221, 5.22, 0],\n",
       " [14.88, 14.57, 0.8811, 5.554, 3.333, 1.018, 4.956, 0],\n",
       " [14.29, 14.09, 0.905, 5.291, 3.337, 2.699, 4.825, 0],\n",
       " [13.84, 13.94, 0.8955, 5.324, 3.379, 2.259, 4.805, 0],\n",
       " [16.14, 14.99, 0.9034, 5.658, 3.562, 1.355, 5.175, 0]]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert the features in dataset from string to float, convert class to integer\n",
    "for i in range(len(dataset[0])-1):\n",
    "    str_column_to_float(dataset, i)\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the minimum and maximum values for each column in the dataset\n",
    "def dataset_minimax(dataset):\n",
    "    minmax = list()\n",
    "    stats = [[min(column),max(column)] for column in zip(*dataset)]\n",
    "    return stats\n",
    "#Rescale the values in the dataset to the range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)-1):\n",
    "            row[i] = (row[i]-minmax[i][0])/(minmax[i][1]-minmax[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.4409820585457979,\n",
       "  0.5020661157024793,\n",
       "  0.570780399274047,\n",
       "  0.48648648648648646,\n",
       "  0.48610121168923714,\n",
       "  0.18930164220052273,\n",
       "  0.3451501723289019,\n",
       "  0],\n",
       " [0.40509915014164316,\n",
       "  0.44628099173553726,\n",
       "  0.6624319419237747,\n",
       "  0.3688063063063065,\n",
       "  0.5010691375623664,\n",
       "  0.03288301759221938,\n",
       "  0.21516494337764666,\n",
       "  0],\n",
       " [0.3493862134088762,\n",
       "  0.3471074380165289,\n",
       "  0.8793103448275864,\n",
       "  0.22072072072072094,\n",
       "  0.50392017106201,\n",
       "  0.25145301590191005,\n",
       "  0.15066469719350079,\n",
       "  0],\n",
       " [0.3068932955618508,\n",
       "  0.31611570247933873,\n",
       "  0.7931034482758617,\n",
       "  0.23930180180180172,\n",
       "  0.5338560228082679,\n",
       "  0.19424254638598865,\n",
       "  0.14081733136386,\n",
       "  0],\n",
       " [0.5240793201133145,\n",
       "  0.5330578512396694,\n",
       "  0.8647912885662429,\n",
       "  0.4273648648648651,\n",
       "  0.6642908054169634,\n",
       "  0.076701036289641,\n",
       "  0.32299359921221066,\n",
       "  0]]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalize features in the dataset\n",
    "minmax = dataset_minimax(dataset)\n",
    "normalize_dataset(dataset,minmax)\n",
    "#Show first five rows of dataset\n",
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split a dataset into k-folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = len(dataset)//n_folds\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold)<fold_size:\n",
    "            index = randrange(len(dataset_copy)) #Get random data\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate accuracy\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i]==predicted[i]:\n",
    "            correct+=1\n",
    "    return correct/float(len(actual))*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate algorithm using cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset,n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set,[])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the activation of one neuron given an input\n",
    "def activate(weights, inputs):\n",
    "    activation = weights[-1] #Bias\n",
    "    for i in range(len(weights)-1):\n",
    "        activation += weights[i]*inputs[i]\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neuron Transfer\n",
    "#Transform the values from the neuron to output\n",
    "#For example sigmoid function\n",
    "def sigmoid(activation):\n",
    "    return 1.0/(1.0+exp(-activation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward propagate input to a network output\n",
    "#Input: network, row(data)\n",
    "def forward_propagate(network, row):\n",
    "    inputs = row #Data as the inputs to the first hidden layer\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        #Iterate through every layer\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'], inputs) #Calculates activation at each neuron in the layer\n",
    "            neuron['output'] = sigmoid(activation) #Calculates output from the activation at each neuron\n",
    "            new_inputs.append(neuron['output']) #The outputs is used as the inputs to the next layer\n",
    "        inputs = new_inputs\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transfer Derivative\n",
    "#Calculate slope from an output value of a neuron\n",
    "def transfer_derivative_sigmoid(output):\n",
    "    return output*(1.0-output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error Backpropagation\n",
    "def backward_propagate_error(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i!= len(network)-1: #For the hidden layer\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i+1]:\n",
    "                    error += (neuron['weights'][j]*neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)): #Iterate through neurons at output layer\n",
    "                neuron = layer[j]\n",
    "                errors.append(expected[j]-neuron['output'])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j]*transfer_derivative_sigmoid(neuron['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update Weights\n",
    "def update_weights(network, row, learning_rate):\n",
    "    #Iterate through layers\n",
    "    for i in range(len(network)): \n",
    "        inputs = row[:-1]\n",
    "        #Except the first hidden layer\n",
    "        if i!=0: \n",
    "            inputs = [neuron['output'] for neuron in network[i-1]] #The inputs is the output from the previous layer\n",
    "        for neuron in network[i]: #Iterate through neurons\n",
    "            for j in range(len(inputs)):\n",
    "                   neuron['weights'][j] += learning_rate*neuron['delta']*inputs[j]\n",
    "            neuron['weights'][-1] += learning_rate*neuron['delta'] #For bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train network\n",
    "def train_network(network, train, learning_rate, n_epoch, n_outputs):\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            expected[row[-1]] = 1\n",
    "            sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, learning_rate)\n",
    "        print('>Epoch=%d, learning_rate=%.3f, error=%.3f' %(epoch, learning_rate, sum_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Has three parameters: the number of inputs (number of features), the number of neurons to have in the hidden layer,\n",
    "#and the number of outputs\n",
    "def initialize_network(n_inputs,n_hidden, n_outputs):\n",
    "    network = list()\n",
    "    #At each neuron in the hidden layer, it has n_inputs+1 weights that is initially randomized between 0 to 1\n",
    "    hidden_layer = [{'weights': [random() for i in range(n_inputs+1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    #At each neuron in the hidden layer, it has n_hidden+1 weights (correspond to each neuron in the hidden layer) \n",
    "    #that is initially randomized between 0 to 1\n",
    "    output_layer = [{'weights': [random() for i in range(n_hidden+1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate prediction\n",
    "def predict(network, row):\n",
    "    outputs = forward_propagate(network, row)\n",
    "    return outputs.index(max(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backpropagation Algorithm with Stochastic Gradient Descent\n",
    "def back_propagation(train, test, learning_rate, n_epoch, n_hidden):\n",
    "    n_inputs = len(train[0])-1\n",
    "    n_outputs = len(set([row[-1] for row in train]))\n",
    "    network = initialize_network(n_inputs, n_hidden, n_outputs)\n",
    "    train_network(network, train, learning_rate, n_epoch, n_outputs)\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        prediction = predict(network, row)\n",
    "        predictions.append(prediction)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Epoch=0, learning_rate=0.300, error=230.469\n",
      ">Epoch=1, learning_rate=0.300, error=155.588\n",
      ">Epoch=2, learning_rate=0.300, error=109.636\n",
      ">Epoch=3, learning_rate=0.300, error=99.606\n",
      ">Epoch=4, learning_rate=0.300, error=85.425\n",
      ">Epoch=5, learning_rate=0.300, error=72.398\n",
      ">Epoch=6, learning_rate=0.300, error=63.877\n",
      ">Epoch=7, learning_rate=0.300, error=58.233\n",
      ">Epoch=8, learning_rate=0.300, error=53.712\n",
      ">Epoch=9, learning_rate=0.300, error=49.494\n",
      ">Epoch=10, learning_rate=0.300, error=45.356\n",
      ">Epoch=11, learning_rate=0.300, error=41.399\n",
      ">Epoch=12, learning_rate=0.300, error=37.826\n",
      ">Epoch=13, learning_rate=0.300, error=34.775\n",
      ">Epoch=14, learning_rate=0.300, error=32.262\n",
      ">Epoch=15, learning_rate=0.300, error=30.225\n",
      ">Epoch=16, learning_rate=0.300, error=28.574\n",
      ">Epoch=17, learning_rate=0.300, error=27.223\n",
      ">Epoch=18, learning_rate=0.300, error=26.103\n",
      ">Epoch=19, learning_rate=0.300, error=25.161\n",
      ">Epoch=20, learning_rate=0.300, error=24.358\n",
      ">Epoch=21, learning_rate=0.300, error=23.664\n",
      ">Epoch=22, learning_rate=0.300, error=23.058\n",
      ">Epoch=23, learning_rate=0.300, error=22.522\n",
      ">Epoch=24, learning_rate=0.300, error=22.045\n",
      ">Epoch=25, learning_rate=0.300, error=21.617\n",
      ">Epoch=26, learning_rate=0.300, error=21.229\n",
      ">Epoch=27, learning_rate=0.300, error=20.876\n",
      ">Epoch=28, learning_rate=0.300, error=20.553\n",
      ">Epoch=29, learning_rate=0.300, error=20.256\n",
      ">Epoch=30, learning_rate=0.300, error=19.981\n",
      ">Epoch=31, learning_rate=0.300, error=19.727\n",
      ">Epoch=32, learning_rate=0.300, error=19.489\n",
      ">Epoch=33, learning_rate=0.300, error=19.267\n",
      ">Epoch=34, learning_rate=0.300, error=19.059\n",
      ">Epoch=35, learning_rate=0.300, error=18.864\n",
      ">Epoch=36, learning_rate=0.300, error=18.679\n",
      ">Epoch=37, learning_rate=0.300, error=18.505\n",
      ">Epoch=38, learning_rate=0.300, error=18.340\n",
      ">Epoch=39, learning_rate=0.300, error=18.184\n",
      ">Epoch=40, learning_rate=0.300, error=18.035\n",
      ">Epoch=41, learning_rate=0.300, error=17.894\n",
      ">Epoch=42, learning_rate=0.300, error=17.758\n",
      ">Epoch=43, learning_rate=0.300, error=17.629\n",
      ">Epoch=44, learning_rate=0.300, error=17.506\n",
      ">Epoch=45, learning_rate=0.300, error=17.387\n",
      ">Epoch=46, learning_rate=0.300, error=17.274\n",
      ">Epoch=47, learning_rate=0.300, error=17.164\n",
      ">Epoch=48, learning_rate=0.300, error=17.059\n",
      ">Epoch=49, learning_rate=0.300, error=16.958\n",
      ">Epoch=50, learning_rate=0.300, error=16.861\n",
      ">Epoch=51, learning_rate=0.300, error=16.767\n",
      ">Epoch=52, learning_rate=0.300, error=16.676\n",
      ">Epoch=53, learning_rate=0.300, error=16.588\n",
      ">Epoch=54, learning_rate=0.300, error=16.503\n",
      ">Epoch=55, learning_rate=0.300, error=16.420\n",
      ">Epoch=56, learning_rate=0.300, error=16.340\n",
      ">Epoch=57, learning_rate=0.300, error=16.263\n",
      ">Epoch=58, learning_rate=0.300, error=16.187\n",
      ">Epoch=59, learning_rate=0.300, error=16.114\n",
      ">Epoch=60, learning_rate=0.300, error=16.043\n",
      ">Epoch=61, learning_rate=0.300, error=15.973\n",
      ">Epoch=62, learning_rate=0.300, error=15.906\n",
      ">Epoch=63, learning_rate=0.300, error=15.840\n",
      ">Epoch=64, learning_rate=0.300, error=15.775\n",
      ">Epoch=65, learning_rate=0.300, error=15.713\n",
      ">Epoch=66, learning_rate=0.300, error=15.651\n",
      ">Epoch=67, learning_rate=0.300, error=15.592\n",
      ">Epoch=68, learning_rate=0.300, error=15.533\n",
      ">Epoch=69, learning_rate=0.300, error=15.476\n",
      ">Epoch=70, learning_rate=0.300, error=15.420\n",
      ">Epoch=71, learning_rate=0.300, error=15.365\n",
      ">Epoch=72, learning_rate=0.300, error=15.311\n",
      ">Epoch=73, learning_rate=0.300, error=15.258\n",
      ">Epoch=74, learning_rate=0.300, error=15.207\n",
      ">Epoch=75, learning_rate=0.300, error=15.156\n",
      ">Epoch=76, learning_rate=0.300, error=15.106\n",
      ">Epoch=77, learning_rate=0.300, error=15.057\n",
      ">Epoch=78, learning_rate=0.300, error=15.009\n",
      ">Epoch=79, learning_rate=0.300, error=14.962\n",
      ">Epoch=80, learning_rate=0.300, error=14.915\n",
      ">Epoch=81, learning_rate=0.300, error=14.870\n",
      ">Epoch=82, learning_rate=0.300, error=14.825\n",
      ">Epoch=83, learning_rate=0.300, error=14.781\n",
      ">Epoch=84, learning_rate=0.300, error=14.737\n",
      ">Epoch=85, learning_rate=0.300, error=14.694\n",
      ">Epoch=86, learning_rate=0.300, error=14.652\n",
      ">Epoch=87, learning_rate=0.300, error=14.610\n",
      ">Epoch=88, learning_rate=0.300, error=14.569\n",
      ">Epoch=89, learning_rate=0.300, error=14.529\n",
      ">Epoch=90, learning_rate=0.300, error=14.489\n",
      ">Epoch=91, learning_rate=0.300, error=14.449\n",
      ">Epoch=92, learning_rate=0.300, error=14.410\n",
      ">Epoch=93, learning_rate=0.300, error=14.372\n",
      ">Epoch=94, learning_rate=0.300, error=14.334\n",
      ">Epoch=95, learning_rate=0.300, error=14.296\n",
      ">Epoch=96, learning_rate=0.300, error=14.259\n",
      ">Epoch=97, learning_rate=0.300, error=14.223\n",
      ">Epoch=98, learning_rate=0.300, error=14.187\n",
      ">Epoch=99, learning_rate=0.300, error=14.151\n",
      ">Epoch=100, learning_rate=0.300, error=14.116\n",
      ">Epoch=101, learning_rate=0.300, error=14.081\n",
      ">Epoch=102, learning_rate=0.300, error=14.046\n",
      ">Epoch=103, learning_rate=0.300, error=14.012\n",
      ">Epoch=104, learning_rate=0.300, error=13.978\n",
      ">Epoch=105, learning_rate=0.300, error=13.944\n",
      ">Epoch=106, learning_rate=0.300, error=13.911\n",
      ">Epoch=107, learning_rate=0.300, error=13.878\n",
      ">Epoch=108, learning_rate=0.300, error=13.845\n",
      ">Epoch=109, learning_rate=0.300, error=13.813\n",
      ">Epoch=110, learning_rate=0.300, error=13.781\n",
      ">Epoch=111, learning_rate=0.300, error=13.749\n",
      ">Epoch=112, learning_rate=0.300, error=13.717\n",
      ">Epoch=113, learning_rate=0.300, error=13.686\n",
      ">Epoch=114, learning_rate=0.300, error=13.655\n",
      ">Epoch=115, learning_rate=0.300, error=13.624\n",
      ">Epoch=116, learning_rate=0.300, error=13.593\n",
      ">Epoch=117, learning_rate=0.300, error=13.563\n",
      ">Epoch=118, learning_rate=0.300, error=13.533\n",
      ">Epoch=119, learning_rate=0.300, error=13.503\n",
      ">Epoch=120, learning_rate=0.300, error=13.473\n",
      ">Epoch=121, learning_rate=0.300, error=13.444\n",
      ">Epoch=122, learning_rate=0.300, error=13.414\n",
      ">Epoch=123, learning_rate=0.300, error=13.385\n",
      ">Epoch=124, learning_rate=0.300, error=13.356\n",
      ">Epoch=125, learning_rate=0.300, error=13.327\n",
      ">Epoch=126, learning_rate=0.300, error=13.298\n",
      ">Epoch=127, learning_rate=0.300, error=13.270\n",
      ">Epoch=128, learning_rate=0.300, error=13.241\n",
      ">Epoch=129, learning_rate=0.300, error=13.213\n",
      ">Epoch=130, learning_rate=0.300, error=13.185\n",
      ">Epoch=131, learning_rate=0.300, error=13.157\n",
      ">Epoch=132, learning_rate=0.300, error=13.129\n",
      ">Epoch=133, learning_rate=0.300, error=13.101\n",
      ">Epoch=134, learning_rate=0.300, error=13.073\n",
      ">Epoch=135, learning_rate=0.300, error=13.045\n",
      ">Epoch=136, learning_rate=0.300, error=13.018\n",
      ">Epoch=137, learning_rate=0.300, error=12.990\n",
      ">Epoch=138, learning_rate=0.300, error=12.963\n",
      ">Epoch=139, learning_rate=0.300, error=12.935\n",
      ">Epoch=140, learning_rate=0.300, error=12.908\n",
      ">Epoch=141, learning_rate=0.300, error=12.881\n",
      ">Epoch=142, learning_rate=0.300, error=12.853\n",
      ">Epoch=143, learning_rate=0.300, error=12.826\n",
      ">Epoch=144, learning_rate=0.300, error=12.799\n",
      ">Epoch=145, learning_rate=0.300, error=12.772\n",
      ">Epoch=146, learning_rate=0.300, error=12.745\n",
      ">Epoch=147, learning_rate=0.300, error=12.718\n",
      ">Epoch=148, learning_rate=0.300, error=12.690\n",
      ">Epoch=149, learning_rate=0.300, error=12.663\n",
      ">Epoch=150, learning_rate=0.300, error=12.636\n",
      ">Epoch=151, learning_rate=0.300, error=12.609\n",
      ">Epoch=152, learning_rate=0.300, error=12.582\n",
      ">Epoch=153, learning_rate=0.300, error=12.555\n",
      ">Epoch=154, learning_rate=0.300, error=12.527\n",
      ">Epoch=155, learning_rate=0.300, error=12.500\n",
      ">Epoch=156, learning_rate=0.300, error=12.473\n",
      ">Epoch=157, learning_rate=0.300, error=12.445\n",
      ">Epoch=158, learning_rate=0.300, error=12.418\n",
      ">Epoch=159, learning_rate=0.300, error=12.390\n",
      ">Epoch=160, learning_rate=0.300, error=12.363\n",
      ">Epoch=161, learning_rate=0.300, error=12.335\n",
      ">Epoch=162, learning_rate=0.300, error=12.307\n",
      ">Epoch=163, learning_rate=0.300, error=12.279\n",
      ">Epoch=164, learning_rate=0.300, error=12.252\n",
      ">Epoch=165, learning_rate=0.300, error=12.224\n",
      ">Epoch=166, learning_rate=0.300, error=12.195\n",
      ">Epoch=167, learning_rate=0.300, error=12.167\n",
      ">Epoch=168, learning_rate=0.300, error=12.139\n",
      ">Epoch=169, learning_rate=0.300, error=12.111\n",
      ">Epoch=170, learning_rate=0.300, error=12.082\n",
      ">Epoch=171, learning_rate=0.300, error=12.054\n",
      ">Epoch=172, learning_rate=0.300, error=12.025\n",
      ">Epoch=173, learning_rate=0.300, error=11.996\n",
      ">Epoch=174, learning_rate=0.300, error=11.968\n",
      ">Epoch=175, learning_rate=0.300, error=11.939\n",
      ">Epoch=176, learning_rate=0.300, error=11.910\n",
      ">Epoch=177, learning_rate=0.300, error=11.881\n",
      ">Epoch=178, learning_rate=0.300, error=11.852\n",
      ">Epoch=179, learning_rate=0.300, error=11.823\n",
      ">Epoch=180, learning_rate=0.300, error=11.794\n",
      ">Epoch=181, learning_rate=0.300, error=11.765\n",
      ">Epoch=182, learning_rate=0.300, error=11.736\n",
      ">Epoch=183, learning_rate=0.300, error=11.707\n",
      ">Epoch=184, learning_rate=0.300, error=11.677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Epoch=185, learning_rate=0.300, error=11.648\n",
      ">Epoch=186, learning_rate=0.300, error=11.619\n",
      ">Epoch=187, learning_rate=0.300, error=11.590\n",
      ">Epoch=188, learning_rate=0.300, error=11.561\n",
      ">Epoch=189, learning_rate=0.300, error=11.532\n",
      ">Epoch=190, learning_rate=0.300, error=11.503\n",
      ">Epoch=191, learning_rate=0.300, error=11.474\n",
      ">Epoch=192, learning_rate=0.300, error=11.446\n",
      ">Epoch=193, learning_rate=0.300, error=11.417\n",
      ">Epoch=194, learning_rate=0.300, error=11.388\n",
      ">Epoch=195, learning_rate=0.300, error=11.360\n",
      ">Epoch=196, learning_rate=0.300, error=11.331\n",
      ">Epoch=197, learning_rate=0.300, error=11.303\n",
      ">Epoch=198, learning_rate=0.300, error=11.275\n",
      ">Epoch=199, learning_rate=0.300, error=11.247\n",
      ">Epoch=200, learning_rate=0.300, error=11.219\n",
      ">Epoch=201, learning_rate=0.300, error=11.191\n",
      ">Epoch=202, learning_rate=0.300, error=11.164\n",
      ">Epoch=203, learning_rate=0.300, error=11.136\n",
      ">Epoch=204, learning_rate=0.300, error=11.109\n",
      ">Epoch=205, learning_rate=0.300, error=11.082\n",
      ">Epoch=206, learning_rate=0.300, error=11.055\n",
      ">Epoch=207, learning_rate=0.300, error=11.028\n",
      ">Epoch=208, learning_rate=0.300, error=11.002\n",
      ">Epoch=209, learning_rate=0.300, error=10.976\n",
      ">Epoch=210, learning_rate=0.300, error=10.949\n",
      ">Epoch=211, learning_rate=0.300, error=10.923\n",
      ">Epoch=212, learning_rate=0.300, error=10.898\n",
      ">Epoch=213, learning_rate=0.300, error=10.872\n",
      ">Epoch=214, learning_rate=0.300, error=10.847\n",
      ">Epoch=215, learning_rate=0.300, error=10.821\n",
      ">Epoch=216, learning_rate=0.300, error=10.796\n",
      ">Epoch=217, learning_rate=0.300, error=10.772\n",
      ">Epoch=218, learning_rate=0.300, error=10.747\n",
      ">Epoch=219, learning_rate=0.300, error=10.723\n",
      ">Epoch=220, learning_rate=0.300, error=10.698\n",
      ">Epoch=221, learning_rate=0.300, error=10.674\n",
      ">Epoch=222, learning_rate=0.300, error=10.651\n",
      ">Epoch=223, learning_rate=0.300, error=10.627\n",
      ">Epoch=224, learning_rate=0.300, error=10.604\n",
      ">Epoch=225, learning_rate=0.300, error=10.580\n",
      ">Epoch=226, learning_rate=0.300, error=10.557\n",
      ">Epoch=227, learning_rate=0.300, error=10.534\n",
      ">Epoch=228, learning_rate=0.300, error=10.512\n",
      ">Epoch=229, learning_rate=0.300, error=10.489\n",
      ">Epoch=230, learning_rate=0.300, error=10.467\n",
      ">Epoch=231, learning_rate=0.300, error=10.445\n",
      ">Epoch=232, learning_rate=0.300, error=10.423\n",
      ">Epoch=233, learning_rate=0.300, error=10.401\n",
      ">Epoch=234, learning_rate=0.300, error=10.379\n",
      ">Epoch=235, learning_rate=0.300, error=10.358\n",
      ">Epoch=236, learning_rate=0.300, error=10.336\n",
      ">Epoch=237, learning_rate=0.300, error=10.315\n",
      ">Epoch=238, learning_rate=0.300, error=10.294\n",
      ">Epoch=239, learning_rate=0.300, error=10.273\n",
      ">Epoch=240, learning_rate=0.300, error=10.253\n",
      ">Epoch=241, learning_rate=0.300, error=10.232\n",
      ">Epoch=242, learning_rate=0.300, error=10.212\n",
      ">Epoch=243, learning_rate=0.300, error=10.191\n",
      ">Epoch=244, learning_rate=0.300, error=10.171\n",
      ">Epoch=245, learning_rate=0.300, error=10.151\n",
      ">Epoch=246, learning_rate=0.300, error=10.131\n",
      ">Epoch=247, learning_rate=0.300, error=10.111\n",
      ">Epoch=248, learning_rate=0.300, error=10.092\n",
      ">Epoch=249, learning_rate=0.300, error=10.072\n",
      ">Epoch=250, learning_rate=0.300, error=10.053\n",
      ">Epoch=251, learning_rate=0.300, error=10.033\n",
      ">Epoch=252, learning_rate=0.300, error=10.014\n",
      ">Epoch=253, learning_rate=0.300, error=9.995\n",
      ">Epoch=254, learning_rate=0.300, error=9.976\n",
      ">Epoch=255, learning_rate=0.300, error=9.957\n",
      ">Epoch=256, learning_rate=0.300, error=9.938\n",
      ">Epoch=257, learning_rate=0.300, error=9.919\n",
      ">Epoch=258, learning_rate=0.300, error=9.900\n",
      ">Epoch=259, learning_rate=0.300, error=9.881\n",
      ">Epoch=260, learning_rate=0.300, error=9.862\n",
      ">Epoch=261, learning_rate=0.300, error=9.844\n",
      ">Epoch=262, learning_rate=0.300, error=9.825\n",
      ">Epoch=263, learning_rate=0.300, error=9.807\n",
      ">Epoch=264, learning_rate=0.300, error=9.788\n",
      ">Epoch=265, learning_rate=0.300, error=9.770\n",
      ">Epoch=266, learning_rate=0.300, error=9.751\n",
      ">Epoch=267, learning_rate=0.300, error=9.733\n",
      ">Epoch=268, learning_rate=0.300, error=9.714\n",
      ">Epoch=269, learning_rate=0.300, error=9.696\n",
      ">Epoch=270, learning_rate=0.300, error=9.678\n",
      ">Epoch=271, learning_rate=0.300, error=9.659\n",
      ">Epoch=272, learning_rate=0.300, error=9.641\n",
      ">Epoch=273, learning_rate=0.300, error=9.623\n",
      ">Epoch=274, learning_rate=0.300, error=9.604\n",
      ">Epoch=275, learning_rate=0.300, error=9.586\n",
      ">Epoch=276, learning_rate=0.300, error=9.568\n",
      ">Epoch=277, learning_rate=0.300, error=9.549\n",
      ">Epoch=278, learning_rate=0.300, error=9.531\n",
      ">Epoch=279, learning_rate=0.300, error=9.512\n",
      ">Epoch=280, learning_rate=0.300, error=9.494\n",
      ">Epoch=281, learning_rate=0.300, error=9.476\n",
      ">Epoch=282, learning_rate=0.300, error=9.457\n",
      ">Epoch=283, learning_rate=0.300, error=9.439\n",
      ">Epoch=284, learning_rate=0.300, error=9.420\n",
      ">Epoch=285, learning_rate=0.300, error=9.402\n",
      ">Epoch=286, learning_rate=0.300, error=9.384\n",
      ">Epoch=287, learning_rate=0.300, error=9.365\n",
      ">Epoch=288, learning_rate=0.300, error=9.347\n",
      ">Epoch=289, learning_rate=0.300, error=9.328\n",
      ">Epoch=290, learning_rate=0.300, error=9.309\n",
      ">Epoch=291, learning_rate=0.300, error=9.291\n",
      ">Epoch=292, learning_rate=0.300, error=9.272\n",
      ">Epoch=293, learning_rate=0.300, error=9.254\n",
      ">Epoch=294, learning_rate=0.300, error=9.235\n",
      ">Epoch=295, learning_rate=0.300, error=9.216\n",
      ">Epoch=296, learning_rate=0.300, error=9.198\n",
      ">Epoch=297, learning_rate=0.300, error=9.179\n",
      ">Epoch=298, learning_rate=0.300, error=9.160\n",
      ">Epoch=299, learning_rate=0.300, error=9.142\n",
      ">Epoch=300, learning_rate=0.300, error=9.123\n",
      ">Epoch=301, learning_rate=0.300, error=9.104\n",
      ">Epoch=302, learning_rate=0.300, error=9.085\n",
      ">Epoch=303, learning_rate=0.300, error=9.067\n",
      ">Epoch=304, learning_rate=0.300, error=9.048\n",
      ">Epoch=305, learning_rate=0.300, error=9.029\n",
      ">Epoch=306, learning_rate=0.300, error=9.010\n",
      ">Epoch=307, learning_rate=0.300, error=8.992\n",
      ">Epoch=308, learning_rate=0.300, error=8.973\n",
      ">Epoch=309, learning_rate=0.300, error=8.954\n",
      ">Epoch=310, learning_rate=0.300, error=8.935\n",
      ">Epoch=311, learning_rate=0.300, error=8.917\n",
      ">Epoch=312, learning_rate=0.300, error=8.898\n",
      ">Epoch=313, learning_rate=0.300, error=8.879\n",
      ">Epoch=314, learning_rate=0.300, error=8.860\n",
      ">Epoch=315, learning_rate=0.300, error=8.842\n",
      ">Epoch=316, learning_rate=0.300, error=8.823\n",
      ">Epoch=317, learning_rate=0.300, error=8.804\n",
      ">Epoch=318, learning_rate=0.300, error=8.785\n",
      ">Epoch=319, learning_rate=0.300, error=8.767\n",
      ">Epoch=320, learning_rate=0.300, error=8.748\n",
      ">Epoch=321, learning_rate=0.300, error=8.729\n",
      ">Epoch=322, learning_rate=0.300, error=8.710\n",
      ">Epoch=323, learning_rate=0.300, error=8.692\n",
      ">Epoch=324, learning_rate=0.300, error=8.673\n",
      ">Epoch=325, learning_rate=0.300, error=8.655\n",
      ">Epoch=326, learning_rate=0.300, error=8.636\n",
      ">Epoch=327, learning_rate=0.300, error=8.617\n",
      ">Epoch=328, learning_rate=0.300, error=8.599\n",
      ">Epoch=329, learning_rate=0.300, error=8.580\n",
      ">Epoch=330, learning_rate=0.300, error=8.561\n",
      ">Epoch=331, learning_rate=0.300, error=8.543\n",
      ">Epoch=332, learning_rate=0.300, error=8.524\n",
      ">Epoch=333, learning_rate=0.300, error=8.506\n",
      ">Epoch=334, learning_rate=0.300, error=8.487\n",
      ">Epoch=335, learning_rate=0.300, error=8.469\n",
      ">Epoch=336, learning_rate=0.300, error=8.450\n",
      ">Epoch=337, learning_rate=0.300, error=8.432\n",
      ">Epoch=338, learning_rate=0.300, error=8.413\n",
      ">Epoch=339, learning_rate=0.300, error=8.395\n",
      ">Epoch=340, learning_rate=0.300, error=8.377\n",
      ">Epoch=341, learning_rate=0.300, error=8.358\n",
      ">Epoch=342, learning_rate=0.300, error=8.340\n",
      ">Epoch=343, learning_rate=0.300, error=8.322\n",
      ">Epoch=344, learning_rate=0.300, error=8.303\n",
      ">Epoch=345, learning_rate=0.300, error=8.285\n",
      ">Epoch=346, learning_rate=0.300, error=8.267\n",
      ">Epoch=347, learning_rate=0.300, error=8.248\n",
      ">Epoch=348, learning_rate=0.300, error=8.230\n",
      ">Epoch=349, learning_rate=0.300, error=8.212\n",
      ">Epoch=350, learning_rate=0.300, error=8.193\n",
      ">Epoch=351, learning_rate=0.300, error=8.175\n",
      ">Epoch=352, learning_rate=0.300, error=8.157\n",
      ">Epoch=353, learning_rate=0.300, error=8.139\n",
      ">Epoch=354, learning_rate=0.300, error=8.121\n",
      ">Epoch=355, learning_rate=0.300, error=8.103\n",
      ">Epoch=356, learning_rate=0.300, error=8.084\n",
      ">Epoch=357, learning_rate=0.300, error=8.066\n",
      ">Epoch=358, learning_rate=0.300, error=8.048\n",
      ">Epoch=359, learning_rate=0.300, error=8.030\n",
      ">Epoch=360, learning_rate=0.300, error=8.012\n",
      ">Epoch=361, learning_rate=0.300, error=7.994\n",
      ">Epoch=362, learning_rate=0.300, error=7.976\n",
      ">Epoch=363, learning_rate=0.300, error=7.958\n",
      ">Epoch=364, learning_rate=0.300, error=7.940\n",
      ">Epoch=365, learning_rate=0.300, error=7.922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Epoch=366, learning_rate=0.300, error=7.904\n",
      ">Epoch=367, learning_rate=0.300, error=7.886\n",
      ">Epoch=368, learning_rate=0.300, error=7.868\n",
      ">Epoch=369, learning_rate=0.300, error=7.850\n",
      ">Epoch=370, learning_rate=0.300, error=7.832\n",
      ">Epoch=371, learning_rate=0.300, error=7.814\n",
      ">Epoch=372, learning_rate=0.300, error=7.796\n",
      ">Epoch=373, learning_rate=0.300, error=7.779\n",
      ">Epoch=374, learning_rate=0.300, error=7.761\n",
      ">Epoch=375, learning_rate=0.300, error=7.743\n",
      ">Epoch=376, learning_rate=0.300, error=7.725\n",
      ">Epoch=377, learning_rate=0.300, error=7.707\n",
      ">Epoch=378, learning_rate=0.300, error=7.690\n",
      ">Epoch=379, learning_rate=0.300, error=7.672\n",
      ">Epoch=380, learning_rate=0.300, error=7.654\n",
      ">Epoch=381, learning_rate=0.300, error=7.636\n",
      ">Epoch=382, learning_rate=0.300, error=7.619\n",
      ">Epoch=383, learning_rate=0.300, error=7.601\n",
      ">Epoch=384, learning_rate=0.300, error=7.583\n",
      ">Epoch=385, learning_rate=0.300, error=7.566\n",
      ">Epoch=386, learning_rate=0.300, error=7.548\n",
      ">Epoch=387, learning_rate=0.300, error=7.530\n",
      ">Epoch=388, learning_rate=0.300, error=7.513\n",
      ">Epoch=389, learning_rate=0.300, error=7.495\n",
      ">Epoch=390, learning_rate=0.300, error=7.478\n",
      ">Epoch=391, learning_rate=0.300, error=7.460\n",
      ">Epoch=392, learning_rate=0.300, error=7.443\n",
      ">Epoch=393, learning_rate=0.300, error=7.425\n",
      ">Epoch=394, learning_rate=0.300, error=7.408\n",
      ">Epoch=395, learning_rate=0.300, error=7.390\n",
      ">Epoch=396, learning_rate=0.300, error=7.373\n",
      ">Epoch=397, learning_rate=0.300, error=7.356\n",
      ">Epoch=398, learning_rate=0.300, error=7.338\n",
      ">Epoch=399, learning_rate=0.300, error=7.321\n",
      ">Epoch=400, learning_rate=0.300, error=7.303\n",
      ">Epoch=401, learning_rate=0.300, error=7.286\n",
      ">Epoch=402, learning_rate=0.300, error=7.269\n",
      ">Epoch=403, learning_rate=0.300, error=7.251\n",
      ">Epoch=404, learning_rate=0.300, error=7.234\n",
      ">Epoch=405, learning_rate=0.300, error=7.217\n",
      ">Epoch=406, learning_rate=0.300, error=7.200\n",
      ">Epoch=407, learning_rate=0.300, error=7.183\n",
      ">Epoch=408, learning_rate=0.300, error=7.165\n",
      ">Epoch=409, learning_rate=0.300, error=7.148\n",
      ">Epoch=410, learning_rate=0.300, error=7.131\n",
      ">Epoch=411, learning_rate=0.300, error=7.114\n",
      ">Epoch=412, learning_rate=0.300, error=7.097\n",
      ">Epoch=413, learning_rate=0.300, error=7.080\n",
      ">Epoch=414, learning_rate=0.300, error=7.063\n",
      ">Epoch=415, learning_rate=0.300, error=7.046\n",
      ">Epoch=416, learning_rate=0.300, error=7.029\n",
      ">Epoch=417, learning_rate=0.300, error=7.012\n",
      ">Epoch=418, learning_rate=0.300, error=6.995\n",
      ">Epoch=419, learning_rate=0.300, error=6.978\n",
      ">Epoch=420, learning_rate=0.300, error=6.961\n",
      ">Epoch=421, learning_rate=0.300, error=6.944\n",
      ">Epoch=422, learning_rate=0.300, error=6.927\n",
      ">Epoch=423, learning_rate=0.300, error=6.910\n",
      ">Epoch=424, learning_rate=0.300, error=6.893\n",
      ">Epoch=425, learning_rate=0.300, error=6.876\n",
      ">Epoch=426, learning_rate=0.300, error=6.860\n",
      ">Epoch=427, learning_rate=0.300, error=6.843\n",
      ">Epoch=428, learning_rate=0.300, error=6.826\n",
      ">Epoch=429, learning_rate=0.300, error=6.809\n",
      ">Epoch=430, learning_rate=0.300, error=6.793\n",
      ">Epoch=431, learning_rate=0.300, error=6.776\n",
      ">Epoch=432, learning_rate=0.300, error=6.759\n",
      ">Epoch=433, learning_rate=0.300, error=6.743\n",
      ">Epoch=434, learning_rate=0.300, error=6.726\n",
      ">Epoch=435, learning_rate=0.300, error=6.709\n",
      ">Epoch=436, learning_rate=0.300, error=6.693\n",
      ">Epoch=437, learning_rate=0.300, error=6.676\n",
      ">Epoch=438, learning_rate=0.300, error=6.660\n",
      ">Epoch=439, learning_rate=0.300, error=6.643\n",
      ">Epoch=440, learning_rate=0.300, error=6.627\n",
      ">Epoch=441, learning_rate=0.300, error=6.610\n",
      ">Epoch=442, learning_rate=0.300, error=6.594\n",
      ">Epoch=443, learning_rate=0.300, error=6.577\n",
      ">Epoch=444, learning_rate=0.300, error=6.561\n",
      ">Epoch=445, learning_rate=0.300, error=6.544\n",
      ">Epoch=446, learning_rate=0.300, error=6.528\n",
      ">Epoch=447, learning_rate=0.300, error=6.512\n",
      ">Epoch=448, learning_rate=0.300, error=6.495\n",
      ">Epoch=449, learning_rate=0.300, error=6.479\n",
      ">Epoch=450, learning_rate=0.300, error=6.463\n",
      ">Epoch=451, learning_rate=0.300, error=6.446\n",
      ">Epoch=452, learning_rate=0.300, error=6.430\n",
      ">Epoch=453, learning_rate=0.300, error=6.414\n",
      ">Epoch=454, learning_rate=0.300, error=6.398\n",
      ">Epoch=455, learning_rate=0.300, error=6.382\n",
      ">Epoch=456, learning_rate=0.300, error=6.365\n",
      ">Epoch=457, learning_rate=0.300, error=6.349\n",
      ">Epoch=458, learning_rate=0.300, error=6.333\n",
      ">Epoch=459, learning_rate=0.300, error=6.317\n",
      ">Epoch=460, learning_rate=0.300, error=6.301\n",
      ">Epoch=461, learning_rate=0.300, error=6.285\n",
      ">Epoch=462, learning_rate=0.300, error=6.269\n",
      ">Epoch=463, learning_rate=0.300, error=6.253\n",
      ">Epoch=464, learning_rate=0.300, error=6.237\n",
      ">Epoch=465, learning_rate=0.300, error=6.221\n",
      ">Epoch=466, learning_rate=0.300, error=6.205\n",
      ">Epoch=467, learning_rate=0.300, error=6.189\n",
      ">Epoch=468, learning_rate=0.300, error=6.173\n",
      ">Epoch=469, learning_rate=0.300, error=6.157\n",
      ">Epoch=470, learning_rate=0.300, error=6.142\n",
      ">Epoch=471, learning_rate=0.300, error=6.126\n",
      ">Epoch=472, learning_rate=0.300, error=6.110\n",
      ">Epoch=473, learning_rate=0.300, error=6.094\n",
      ">Epoch=474, learning_rate=0.300, error=6.078\n",
      ">Epoch=475, learning_rate=0.300, error=6.063\n",
      ">Epoch=476, learning_rate=0.300, error=6.047\n",
      ">Epoch=477, learning_rate=0.300, error=6.031\n",
      ">Epoch=478, learning_rate=0.300, error=6.016\n",
      ">Epoch=479, learning_rate=0.300, error=6.000\n",
      ">Epoch=480, learning_rate=0.300, error=5.985\n",
      ">Epoch=481, learning_rate=0.300, error=5.969\n",
      ">Epoch=482, learning_rate=0.300, error=5.954\n",
      ">Epoch=483, learning_rate=0.300, error=5.938\n",
      ">Epoch=484, learning_rate=0.300, error=5.923\n",
      ">Epoch=485, learning_rate=0.300, error=5.907\n",
      ">Epoch=486, learning_rate=0.300, error=5.892\n",
      ">Epoch=487, learning_rate=0.300, error=5.876\n",
      ">Epoch=488, learning_rate=0.300, error=5.861\n",
      ">Epoch=489, learning_rate=0.300, error=5.846\n",
      ">Epoch=490, learning_rate=0.300, error=5.830\n",
      ">Epoch=491, learning_rate=0.300, error=5.815\n",
      ">Epoch=492, learning_rate=0.300, error=5.800\n",
      ">Epoch=493, learning_rate=0.300, error=5.785\n",
      ">Epoch=494, learning_rate=0.300, error=5.769\n",
      ">Epoch=495, learning_rate=0.300, error=5.754\n",
      ">Epoch=496, learning_rate=0.300, error=5.739\n",
      ">Epoch=497, learning_rate=0.300, error=5.724\n",
      ">Epoch=498, learning_rate=0.300, error=5.709\n",
      ">Epoch=499, learning_rate=0.300, error=5.694\n",
      "Scores: [90.47619047619048]\n",
      "Mean Accuracy: 90.476%\n"
     ]
    }
   ],
   "source": [
    "seed(1)\n",
    "#Evaluate algorithm\n",
    "n_folds = 5\n",
    "learning_rate = 0.3\n",
    "n_epoch = 500\n",
    "n_hidden = 10\n",
    "scores = evaluate_algorithm(dataset, back_propagation, n_folds, learning_rate, n_epoch, n_hidden)\n",
    "print('Scores: %s' %scores)\n",
    "print('Mean Accuracy: %.3f%%' %(sum(scores)/float(len(scores))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
