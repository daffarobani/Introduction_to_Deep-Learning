{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Tune Batch Size and Number of Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from numpy import loadtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "dataset = loadtxt(r'C:\\Users\\LEGION\\Machine_Learning_Learn_Project\\Intro_Deep_Learning\\pima-indians-diabetes.csv', delimiter=',')\n",
    "#Split into X and y\n",
    "nvar = dataset.shape[1]-1\n",
    "X = dataset[:,:nvar]\n",
    "y = dataset[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "77/77 [==============================] - 0s 721us/step - loss: 17.3865 - accuracy: 0.6492\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 0s 736us/step - loss: 5.2125 - accuracy: 0.5561\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 0s 694us/step - loss: 3.9871 - accuracy: 0.5395\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 3.2285 - accuracy: 0.514 - 0s 772us/step - loss: 3.1606 - accuracy: 0.5149\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 0s 731us/step - loss: 2.3247 - accuracy: 0.5241\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - 0s 712us/step - loss: 2.1751 - accuracy: 0.5357\n",
      "Epoch 7/100\n",
      "77/77 [==============================] - 0s 706us/step - loss: 1.6171 - accuracy: 0.5621\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - 0s 698us/step - loss: 1.6466 - accuracy: 0.5898\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - 0s 684us/step - loss: 1.4805 - accuracy: 0.6006\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - 0s 633us/step - loss: 1.4409 - accuracy: 0.6245\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - 0s 668us/step - loss: 1.3630 - accuracy: 0.6078\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - 0s 702us/step - loss: 1.0824 - accuracy: 0.6308\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - 0s 673us/step - loss: 1.0275 - accuracy: 0.6601\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - 0s 707us/step - loss: 1.0847 - accuracy: 0.6268\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - 0s 698us/step - loss: 1.0498 - accuracy: 0.6340\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - 0s 690us/step - loss: 0.9258 - accuracy: 0.6332\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - 0s 623us/step - loss: 0.8921 - accuracy: 0.6524\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - 0s 627us/step - loss: 0.8932 - accuracy: 0.6392\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - 0s 591us/step - loss: 0.8618 - accuracy: 0.6239\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - 0s 656us/step - loss: 0.8375 - accuracy: 0.6450\n",
      "Epoch 21/100\n",
      "77/77 [==============================] - 0s 699us/step - loss: 0.7945 - accuracy: 0.6623\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - 0s 665us/step - loss: 0.8107 - accuracy: 0.6201\n",
      "Epoch 23/100\n",
      "77/77 [==============================] - 0s 693us/step - loss: 0.7201 - accuracy: 0.6782\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - 0s 651us/step - loss: 0.6801 - accuracy: 0.7044\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - 0s 643us/step - loss: 0.6940 - accuracy: 0.6858\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - 0s 711us/step - loss: 0.6576 - accuracy: 0.6920\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - 0s 647us/step - loss: 0.6729 - accuracy: 0.6723\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - 0s 648us/step - loss: 0.6202 - accuracy: 0.6983\n",
      "Epoch 29/100\n",
      "77/77 [==============================] - 0s 710us/step - loss: 0.5982 - accuracy: 0.7116\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - 0s 617us/step - loss: 0.6584 - accuracy: 0.6750\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - 0s 660us/step - loss: 0.6602 - accuracy: 0.6745\n",
      "Epoch 32/100\n",
      "77/77 [==============================] - 0s 591us/step - loss: 0.6126 - accuracy: 0.7017\n",
      "Epoch 33/100\n",
      "77/77 [==============================] - 0s 642us/step - loss: 0.6994 - accuracy: 0.6733\n",
      "Epoch 34/100\n",
      "77/77 [==============================] - 0s 643us/step - loss: 0.5915 - accuracy: 0.7131\n",
      "Epoch 35/100\n",
      "77/77 [==============================] - 0s 538us/step - loss: 0.6149 - accuracy: 0.6918\n",
      "Epoch 36/100\n",
      "77/77 [==============================] - 0s 560us/step - loss: 0.6059 - accuracy: 0.6920\n",
      "Epoch 37/100\n",
      "77/77 [==============================] - 0s 577us/step - loss: 0.6684 - accuracy: 0.6623\n",
      "Epoch 38/100\n",
      "77/77 [==============================] - 0s 591us/step - loss: 0.5773 - accuracy: 0.7242\n",
      "Epoch 39/100\n",
      "77/77 [==============================] - 0s 538us/step - loss: 0.6482 - accuracy: 0.6617\n",
      "Epoch 40/100\n",
      "77/77 [==============================] - 0s 686us/step - loss: 0.5785 - accuracy: 0.7094\n",
      "Epoch 41/100\n",
      "77/77 [==============================] - 0s 853us/step - loss: 0.5649 - accuracy: 0.7140\n",
      "Epoch 42/100\n",
      "77/77 [==============================] - 0s 906us/step - loss: 0.5968 - accuracy: 0.6990\n",
      "Epoch 43/100\n",
      "77/77 [==============================] - 0s 973us/step - loss: 0.6253 - accuracy: 0.6882\n",
      "Epoch 44/100\n",
      "77/77 [==============================] - 0s 981us/step - loss: 0.5514 - accuracy: 0.7434\n",
      "Epoch 45/100\n",
      "77/77 [==============================] - 0s 853us/step - loss: 0.5898 - accuracy: 0.7065\n",
      "Epoch 46/100\n",
      "77/77 [==============================] - 0s 682us/step - loss: 0.5417 - accuracy: 0.7311\n",
      "Epoch 47/100\n",
      "77/77 [==============================] - 0s 656us/step - loss: 0.5807 - accuracy: 0.6679\n",
      "Epoch 48/100\n",
      "77/77 [==============================] - 0s 715us/step - loss: 0.5634 - accuracy: 0.7107\n",
      "Epoch 49/100\n",
      "77/77 [==============================] - 0s 699us/step - loss: 0.5790 - accuracy: 0.7151\n",
      "Epoch 50/100\n",
      "77/77 [==============================] - 0s 806us/step - loss: 0.5849 - accuracy: 0.7295\n",
      "Epoch 51/100\n",
      "77/77 [==============================] - 0s 915us/step - loss: 0.5942 - accuracy: 0.7209\n",
      "Epoch 52/100\n",
      "77/77 [==============================] - 0s 797us/step - loss: 0.5594 - accuracy: 0.7161\n",
      "Epoch 53/100\n",
      "77/77 [==============================] - 0s 731us/step - loss: 0.5769 - accuracy: 0.7246\n",
      "Epoch 54/100\n",
      "77/77 [==============================] - 0s 734us/step - loss: 0.5783 - accuracy: 0.7139\n",
      "Epoch 55/100\n",
      "77/77 [==============================] - 0s 770us/step - loss: 0.5498 - accuracy: 0.7363\n",
      "Epoch 56/100\n",
      "77/77 [==============================] - 0s 800us/step - loss: 0.5156 - accuracy: 0.7445\n",
      "Epoch 57/100\n",
      "77/77 [==============================] - 0s 845us/step - loss: 0.5428 - accuracy: 0.7357\n",
      "Epoch 58/100\n",
      "77/77 [==============================] - 0s 780us/step - loss: 0.5955 - accuracy: 0.7035\n",
      "Epoch 59/100\n",
      "77/77 [==============================] - 0s 691us/step - loss: 0.5504 - accuracy: 0.7260\n",
      "Epoch 60/100\n",
      "77/77 [==============================] - 0s 649us/step - loss: 0.5520 - accuracy: 0.7341\n",
      "Epoch 61/100\n",
      "77/77 [==============================] - 0s 676us/step - loss: 0.5453 - accuracy: 0.7020\n",
      "Epoch 62/100\n",
      "77/77 [==============================] - 0s 738us/step - loss: 0.5524 - accuracy: 0.7236\n",
      "Epoch 63/100\n",
      "77/77 [==============================] - 0s 880us/step - loss: 0.6025 - accuracy: 0.7035\n",
      "Epoch 64/100\n",
      "77/77 [==============================] - 0s 853us/step - loss: 0.5609 - accuracy: 0.6871\n",
      "Epoch 65/100\n",
      "77/77 [==============================] - 0s 812us/step - loss: 0.5701 - accuracy: 0.7351\n",
      "Epoch 66/100\n",
      "77/77 [==============================] - 0s 615us/step - loss: 0.5603 - accuracy: 0.7169\n",
      "Epoch 67/100\n",
      "77/77 [==============================] - 0s 784us/step - loss: 0.5618 - accuracy: 0.7021\n",
      "Epoch 68/100\n",
      "77/77 [==============================] - 0s 696us/step - loss: 0.6106 - accuracy: 0.6874\n",
      "Epoch 69/100\n",
      "77/77 [==============================] - 0s 743us/step - loss: 0.5615 - accuracy: 0.7383\n",
      "Epoch 70/100\n",
      "77/77 [==============================] - 0s 862us/step - loss: 0.5432 - accuracy: 0.7250\n",
      "Epoch 71/100\n",
      "77/77 [==============================] - 0s 837us/step - loss: 0.5595 - accuracy: 0.7133\n",
      "Epoch 72/100\n",
      "77/77 [==============================] - 0s 934us/step - loss: 0.5832 - accuracy: 0.7239\n",
      "Epoch 73/100\n",
      "77/77 [==============================] - 0s 717us/step - loss: 0.5616 - accuracy: 0.7470\n",
      "Epoch 74/100\n",
      "77/77 [==============================] - 0s 647us/step - loss: 0.5488 - accuracy: 0.7342\n",
      "Epoch 75/100\n",
      "77/77 [==============================] - 0s 676us/step - loss: 0.5624 - accuracy: 0.7073\n",
      "Epoch 76/100\n",
      "77/77 [==============================] - 0s 671us/step - loss: 0.5475 - accuracy: 0.7310\n",
      "Epoch 77/100\n",
      "77/77 [==============================] - 0s 846us/step - loss: 0.5884 - accuracy: 0.7106\n",
      "Epoch 78/100\n",
      "77/77 [==============================] - 0s 861us/step - loss: 0.5680 - accuracy: 0.7272\n",
      "Epoch 79/100\n",
      "77/77 [==============================] - 0s 791us/step - loss: 0.5430 - accuracy: 0.7227\n",
      "Epoch 80/100\n",
      "77/77 [==============================] - 0s 645us/step - loss: 0.5431 - accuracy: 0.7066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100\n",
      "77/77 [==============================] - 0s 717us/step - loss: 0.5613 - accuracy: 0.7242\n",
      "Epoch 82/100\n",
      "77/77 [==============================] - 0s 656us/step - loss: 0.5581 - accuracy: 0.7112\n",
      "Epoch 83/100\n",
      "77/77 [==============================] - 0s 635us/step - loss: 0.5495 - accuracy: 0.7417\n",
      "Epoch 84/100\n",
      "77/77 [==============================] - 0s 774us/step - loss: 0.5060 - accuracy: 0.7491\n",
      "Epoch 85/100\n",
      "77/77 [==============================] - 0s 825us/step - loss: 0.5635 - accuracy: 0.7170\n",
      "Epoch 86/100\n",
      "77/77 [==============================] - 0s 822us/step - loss: 0.5369 - accuracy: 0.7370\n",
      "Epoch 87/100\n",
      "77/77 [==============================] - 0s 864us/step - loss: 0.5490 - accuracy: 0.7293\n",
      "Epoch 88/100\n",
      "77/77 [==============================] - 0s 695us/step - loss: 0.5331 - accuracy: 0.7560\n",
      "Epoch 89/100\n",
      "77/77 [==============================] - 0s 699us/step - loss: 0.5502 - accuracy: 0.7188\n",
      "Epoch 90/100\n",
      "77/77 [==============================] - 0s 779us/step - loss: 0.5136 - accuracy: 0.7624\n",
      "Epoch 91/100\n",
      "77/77 [==============================] - 0s 812us/step - loss: 0.5352 - accuracy: 0.7237\n",
      "Epoch 92/100\n",
      "77/77 [==============================] - 0s 880us/step - loss: 0.5053 - accuracy: 0.7559\n",
      "Epoch 93/100\n",
      "77/77 [==============================] - 0s 836us/step - loss: 0.5483 - accuracy: 0.7301\n",
      "Epoch 94/100\n",
      "77/77 [==============================] - 0s 717us/step - loss: 0.5494 - accuracy: 0.71810s - loss: 0.5502 - accuracy: 0.71\n",
      "Epoch 95/100\n",
      "77/77 [==============================] - 0s 703us/step - loss: 0.5645 - accuracy: 0.7211\n",
      "Epoch 96/100\n",
      "77/77 [==============================] - 0s 696us/step - loss: 0.5451 - accuracy: 0.7460\n",
      "Epoch 97/100\n",
      "77/77 [==============================] - 0s 821us/step - loss: 0.4976 - accuracy: 0.7564\n",
      "Epoch 98/100\n",
      "77/77 [==============================] - 0s 849us/step - loss: 0.5280 - accuracy: 0.7371\n",
      "Epoch 99/100\n",
      "77/77 [==============================] - 0s 792us/step - loss: 0.5348 - accuracy: 0.7281\n",
      "Epoch 100/100\n",
      "77/77 [==============================] - 0s 822us/step - loss: 0.5206 - accuracy: 0.7209\n",
      "Best: 0.723958 using {'batch_size': 10, 'epochs': 100}\n",
      "Mean: 0.613281, Std: 0.051329 with: {'batch_size': 10, 'epochs': 10}\n",
      "Mean: 0.651042, Std: 0.015733 with: {'batch_size': 10, 'epochs': 50}\n",
      "Mean: 0.723958, Std: 0.022402 with: {'batch_size': 10, 'epochs': 100}\n",
      "Mean: 0.572917, Std: 0.008027 with: {'batch_size': 20, 'epochs': 10}\n",
      "Mean: 0.684896, Std: 0.009744 with: {'batch_size': 20, 'epochs': 50}\n",
      "Mean: 0.661458, Std: 0.023939 with: {'batch_size': 20, 'epochs': 100}\n",
      "Mean: 0.597656, Std: 0.038273 with: {'batch_size': 40, 'epochs': 10}\n",
      "Mean: 0.644531, Std: 0.011049 with: {'batch_size': 40, 'epochs': 50}\n",
      "Mean: 0.687500, Std: 0.019918 with: {'batch_size': 40, 'epochs': 100}\n",
      "Mean: 0.506510, Std: 0.100117 with: {'batch_size': 60, 'epochs': 10}\n",
      "Mean: 0.628906, Std: 0.017758 with: {'batch_size': 60, 'epochs': 50}\n",
      "Mean: 0.670573, Std: 0.011201 with: {'batch_size': 60, 'epochs': 100}\n",
      "Mean: 0.440104, Std: 0.034401 with: {'batch_size': 80, 'epochs': 10}\n",
      "Mean: 0.565104, Std: 0.062201 with: {'batch_size': 80, 'epochs': 50}\n",
      "Mean: 0.665365, Std: 0.021236 with: {'batch_size': 80, 'epochs': 100}\n",
      "Mean: 0.613281, Std: 0.030425 with: {'batch_size': 100, 'epochs': 10}\n",
      "Mean: 0.645833, Std: 0.050865 with: {'batch_size': 100, 'epochs': 50}\n",
      "Mean: 0.602865, Std: 0.046694 with: {'batch_size': 100, 'epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "#Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    #Create model\n",
    "    model = Sequential()\n",
    "    #Hidden layer\n",
    "    model.add(Dense(12,input_dim=X.shape[1], activation='relu'))\n",
    "    #Output Layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    #Compile Model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "#Fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "#Create model\n",
    "model = KerasClassifier(build_fn= create_model, verbose=1)\n",
    "#Define grid search parameters\n",
    "batch_size=[10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "#Use 3 fold Cross Validation\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X,y)\n",
    "#Summarize results\n",
    "print(\"Best: %f using %s\" %(grid_result.best_score_, grid_result.best_params_))\n",
    "#Result at every parameter combination\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"Mean: %f, Std: %f with: %r\" %(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The optimal number of epochs:100, batch_size: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Tune the Training Optimization Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "77/77 [==============================] - 0s 664us/step - loss: 24.1188 - accuracy: 0.6632\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 0s 656us/step - loss: 10.4205 - accuracy: 0.6266\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 0s 713us/step - loss: 4.2171 - accuracy: 0.5882\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - 0s 673us/step - loss: 3.2429 - accuracy: 0.5978\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 0s 591us/step - loss: 2.4460 - accuracy: 0.6775\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - 0s 705us/step - loss: 1.7219 - accuracy: 0.6602\n",
      "Epoch 7/100\n",
      "77/77 [==============================] - 0s 630us/step - loss: 1.6239 - accuracy: 0.6290\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - 0s 654us/step - loss: 1.5090 - accuracy: 0.6585\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - 0s 656us/step - loss: 1.1371 - accuracy: 0.6848\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - 0s 609us/step - loss: 0.9690 - accuracy: 0.6927\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - 0s 766us/step - loss: 0.8858 - accuracy: 0.6690\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - 0s 722us/step - loss: 0.9281 - accuracy: 0.6595\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - 0s 657us/step - loss: 0.7803 - accuracy: 0.7149\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - 0s 626us/step - loss: 0.7888 - accuracy: 0.6978\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - 0s 632us/step - loss: 0.7578 - accuracy: 0.6849\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - 0s 717us/step - loss: 0.7879 - accuracy: 0.6504\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - 0s 695us/step - loss: 0.7123 - accuracy: 0.6772\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - 0s 659us/step - loss: 0.6640 - accuracy: 0.6935\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - 0s 648us/step - loss: 0.7982 - accuracy: 0.6479\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - 0s 626us/step - loss: 0.6446 - accuracy: 0.6920\n",
      "Epoch 21/100\n",
      "77/77 [==============================] - 0s 622us/step - loss: 0.6403 - accuracy: 0.6952\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - 0s 723us/step - loss: 0.7099 - accuracy: 0.6608\n",
      "Epoch 23/100\n",
      "77/77 [==============================] - 0s 712us/step - loss: 0.6117 - accuracy: 0.7114\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - 0s 715us/step - loss: 0.6286 - accuracy: 0.6698\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - 0s 682us/step - loss: 0.6069 - accuracy: 0.7195\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - 0s 634us/step - loss: 0.6419 - accuracy: 0.6703\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - 0s 618us/step - loss: 0.6031 - accuracy: 0.7235\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - 0s 695us/step - loss: 0.5773 - accuracy: 0.7190\n",
      "Epoch 29/100\n",
      "77/77 [==============================] - 0s 569us/step - loss: 0.6123 - accuracy: 0.7017\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - 0s 678us/step - loss: 0.5830 - accuracy: 0.7358\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - 0s 639us/step - loss: 0.5736 - accuracy: 0.7203\n",
      "Epoch 32/100\n",
      "77/77 [==============================] - 0s 652us/step - loss: 0.6089 - accuracy: 0.6989\n",
      "Epoch 33/100\n",
      "77/77 [==============================] - 0s 694us/step - loss: 0.6294 - accuracy: 0.7065\n",
      "Epoch 34/100\n",
      "77/77 [==============================] - 0s 709us/step - loss: 0.6014 - accuracy: 0.7118\n",
      "Epoch 35/100\n",
      "77/77 [==============================] - 0s 755us/step - loss: 0.6402 - accuracy: 0.6710\n",
      "Epoch 36/100\n",
      "77/77 [==============================] - 0s 673us/step - loss: 0.6168 - accuracy: 0.7122\n",
      "Epoch 37/100\n",
      "77/77 [==============================] - 0s 656us/step - loss: 0.5969 - accuracy: 0.7192\n",
      "Epoch 38/100\n",
      "77/77 [==============================] - 0s 689us/step - loss: 0.5745 - accuracy: 0.7094\n",
      "Epoch 39/100\n",
      "77/77 [==============================] - 0s 755us/step - loss: 0.6093 - accuracy: 0.6964\n",
      "Epoch 40/100\n",
      "77/77 [==============================] - 0s 668us/step - loss: 0.5562 - accuracy: 0.6959\n",
      "Epoch 41/100\n",
      "77/77 [==============================] - 0s 712us/step - loss: 0.6225 - accuracy: 0.6828\n",
      "Epoch 42/100\n",
      "77/77 [==============================] - 0s 667us/step - loss: 0.5749 - accuracy: 0.7140\n",
      "Epoch 43/100\n",
      "77/77 [==============================] - 0s 634us/step - loss: 0.6093 - accuracy: 0.7023\n",
      "Epoch 44/100\n",
      "77/77 [==============================] - 0s 644us/step - loss: 0.5532 - accuracy: 0.7303\n",
      "Epoch 45/100\n",
      "77/77 [==============================] - 0s 701us/step - loss: 0.5312 - accuracy: 0.7434\n",
      "Epoch 46/100\n",
      "77/77 [==============================] - 0s 699us/step - loss: 0.5699 - accuracy: 0.7307\n",
      "Epoch 47/100\n",
      "77/77 [==============================] - 0s 630us/step - loss: 0.5650 - accuracy: 0.7424\n",
      "Epoch 48/100\n",
      "77/77 [==============================] - 0s 636us/step - loss: 0.5436 - accuracy: 0.7278\n",
      "Epoch 49/100\n",
      "77/77 [==============================] - 0s 632us/step - loss: 0.5208 - accuracy: 0.7483\n",
      "Epoch 50/100\n",
      "77/77 [==============================] - 0s 682us/step - loss: 0.5611 - accuracy: 0.7037\n",
      "Epoch 51/100\n",
      "77/77 [==============================] - 0s 677us/step - loss: 0.5952 - accuracy: 0.6984\n",
      "Epoch 52/100\n",
      "77/77 [==============================] - 0s 649us/step - loss: 0.6620 - accuracy: 0.6786\n",
      "Epoch 53/100\n",
      "77/77 [==============================] - 0s 696us/step - loss: 0.5624 - accuracy: 0.7372\n",
      "Epoch 54/100\n",
      "77/77 [==============================] - 0s 656us/step - loss: 0.5572 - accuracy: 0.7183\n",
      "Epoch 55/100\n",
      "77/77 [==============================] - 0s 630us/step - loss: 0.5967 - accuracy: 0.7254\n",
      "Epoch 56/100\n",
      "77/77 [==============================] - 0s 630us/step - loss: 0.5474 - accuracy: 0.7376\n",
      "Epoch 57/100\n",
      "77/77 [==============================] - 0s 604us/step - loss: 0.5216 - accuracy: 0.7564\n",
      "Epoch 58/100\n",
      "77/77 [==============================] - 0s 538us/step - loss: 0.6285 - accuracy: 0.7134\n",
      "Epoch 59/100\n",
      "77/77 [==============================] - 0s 538us/step - loss: 0.5241 - accuracy: 0.7301\n",
      "Epoch 60/100\n",
      "77/77 [==============================] - 0s 685us/step - loss: 0.5257 - accuracy: 0.7438\n",
      "Epoch 61/100\n",
      "77/77 [==============================] - 0s 564us/step - loss: 0.5261 - accuracy: 0.7414\n",
      "Epoch 62/100\n",
      "77/77 [==============================] - 0s 643us/step - loss: 0.5462 - accuracy: 0.7479\n",
      "Epoch 63/100\n",
      "77/77 [==============================] - 0s 617us/step - loss: 0.5499 - accuracy: 0.7411\n",
      "Epoch 64/100\n",
      "77/77 [==============================] - 0s 761us/step - loss: 0.5369 - accuracy: 0.7397\n",
      "Epoch 65/100\n",
      "77/77 [==============================] - 0s 785us/step - loss: 0.5640 - accuracy: 0.7317\n",
      "Epoch 66/100\n",
      "77/77 [==============================] - 0s 838us/step - loss: 0.5863 - accuracy: 0.7244\n",
      "Epoch 67/100\n",
      "77/77 [==============================] - 0s 817us/step - loss: 0.5024 - accuracy: 0.7752\n",
      "Epoch 68/100\n",
      "77/77 [==============================] - 0s 762us/step - loss: 0.5282 - accuracy: 0.7513\n",
      "Epoch 69/100\n",
      "77/77 [==============================] - 0s 682us/step - loss: 0.5169 - accuracy: 0.7677\n",
      "Epoch 70/100\n",
      "77/77 [==============================] - 0s 608us/step - loss: 0.5342 - accuracy: 0.7414\n",
      "Epoch 71/100\n",
      "77/77 [==============================] - 0s 705us/step - loss: 0.5531 - accuracy: 0.7382\n",
      "Epoch 72/100\n",
      "77/77 [==============================] - 0s 657us/step - loss: 0.4979 - accuracy: 0.7392\n",
      "Epoch 73/100\n",
      "77/77 [==============================] - 0s 957us/step - loss: 0.5833 - accuracy: 0.7175\n",
      "Epoch 74/100\n",
      "77/77 [==============================] - 0s 840us/step - loss: 0.5636 - accuracy: 0.7302\n",
      "Epoch 75/100\n",
      "77/77 [==============================] - 0s 886us/step - loss: 0.4975 - accuracy: 0.7933\n",
      "Epoch 76/100\n",
      "77/77 [==============================] - 0s 711us/step - loss: 0.5450 - accuracy: 0.7312\n",
      "Epoch 77/100\n",
      "77/77 [==============================] - 0s 718us/step - loss: 0.5528 - accuracy: 0.7317\n",
      "Epoch 78/100\n",
      "77/77 [==============================] - 0s 682us/step - loss: 0.5118 - accuracy: 0.7712\n",
      "Epoch 79/100\n",
      "77/77 [==============================] - 0s 635us/step - loss: 0.5476 - accuracy: 0.7009\n",
      "Epoch 80/100\n",
      "77/77 [==============================] - 0s 704us/step - loss: 0.5171 - accuracy: 0.7463\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 0s 696us/step - loss: 0.5525 - accuracy: 0.7278\n",
      "Epoch 82/100\n",
      "77/77 [==============================] - 0s 707us/step - loss: 0.6219 - accuracy: 0.6871\n",
      "Epoch 83/100\n",
      "77/77 [==============================] - 0s 684us/step - loss: 0.5288 - accuracy: 0.7474\n",
      "Epoch 84/100\n",
      "77/77 [==============================] - 0s 617us/step - loss: 0.5288 - accuracy: 0.7527\n",
      "Epoch 85/100\n",
      "77/77 [==============================] - 0s 579us/step - loss: 0.5285 - accuracy: 0.7649\n",
      "Epoch 86/100\n",
      "77/77 [==============================] - 0s 622us/step - loss: 0.5380 - accuracy: 0.7226\n",
      "Epoch 87/100\n",
      "77/77 [==============================] - 0s 637us/step - loss: 0.5400 - accuracy: 0.7598\n",
      "Epoch 88/100\n",
      "77/77 [==============================] - 0s 709us/step - loss: 0.5051 - accuracy: 0.7459\n",
      "Epoch 89/100\n",
      "77/77 [==============================] - 0s 827us/step - loss: 0.5422 - accuracy: 0.7347\n",
      "Epoch 90/100\n",
      "77/77 [==============================] - 0s 787us/step - loss: 0.5109 - accuracy: 0.7499\n",
      "Epoch 91/100\n",
      "77/77 [==============================] - 0s 761us/step - loss: 0.5149 - accuracy: 0.7365\n",
      "Epoch 92/100\n",
      "77/77 [==============================] - 0s 879us/step - loss: 0.5601 - accuracy: 0.7294\n",
      "Epoch 93/100\n",
      "77/77 [==============================] - 0s 879us/step - loss: 0.4875 - accuracy: 0.7746\n",
      "Epoch 94/100\n",
      "77/77 [==============================] - 0s 814us/step - loss: 0.5775 - accuracy: 0.7179\n",
      "Epoch 95/100\n",
      "77/77 [==============================] - 0s 840us/step - loss: 0.5046 - accuracy: 0.7685\n",
      "Epoch 96/100\n",
      "77/77 [==============================] - 0s 645us/step - loss: 0.5416 - accuracy: 0.7386\n",
      "Epoch 97/100\n",
      "77/77 [==============================] - 0s 683us/step - loss: 0.5358 - accuracy: 0.7340\n",
      "Epoch 98/100\n",
      "77/77 [==============================] - 0s 687us/step - loss: 0.5579 - accuracy: 0.7340\n",
      "Epoch 99/100\n",
      "77/77 [==============================] - 0s 639us/step - loss: 0.5123 - accuracy: 0.7561\n",
      "Epoch 100/100\n",
      "77/77 [==============================] - 0s 645us/step - loss: 0.6411 - accuracy: 0.6950\n",
      "Best: 0.704427 using {'optimizer': 'Adam'}\n",
      "Mean: 0.635417, Std: 0.023939 with: {'optimizer': 'SGD'}\n",
      "Mean: 0.675781, Std: 0.046983 with: {'optimizer': 'RMSprop'}\n",
      "Mean: 0.500000, Std: 0.011500 with: {'optimizer': 'Adagrad'}\n",
      "Mean: 0.472656, Std: 0.100252 with: {'optimizer': 'Adadelta'}\n",
      "Mean: 0.704427, Std: 0.023939 with: {'optimizer': 'Adam'}\n",
      "Mean: 0.682292, Std: 0.034401 with: {'optimizer': 'Adamax'}\n",
      "Mean: 0.704427, Std: 0.016367 with: {'optimizer': 'Nadam'}\n"
     ]
    }
   ],
   "source": [
    "def create_model(optimizer='adam'):\n",
    "    #Create model\n",
    "    model = Sequential()\n",
    "    #Hidden layer\n",
    "    model.add(Dense(12,input_dim=X.shape[1], activation='relu'))\n",
    "    #Output Layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    #Compile Model\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#Fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "model = KerasClassifier(build_fn = create_model, epochs=100, batch_size=10, verbose=1)\n",
    "#Define grid search parameters\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "param_grid = dict(optimizer=optimizer)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X,y)\n",
    "\n",
    "#Summarize results\n",
    "print(\"Best: %f using %s\" %(grid_result.best_score_, grid_result.best_params_))\n",
    "#Result at every parameter combination\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"Mean: %f, Std: %f with: %r\" %(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adam is found to be the best optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Tune Learning Rate and Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "77/77 [==============================] - 0s 761us/step - loss: 5.2748 - accuracy: 0.6065\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 0s 814us/step - loss: 1.2419 - accuracy: 0.5968\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 0s 905us/step - loss: 1.2966 - accuracy: 0.5427\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - 0s 866us/step - loss: 0.7610 - accuracy: 0.6608\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 0s 889us/step - loss: 0.7348 - accuracy: 0.6398\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - 0s 814us/step - loss: 0.7160 - accuracy: 0.6573\n",
      "Epoch 7/100\n",
      "77/77 [==============================] - 0s 696us/step - loss: 0.6894 - accuracy: 0.6750\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - 0s 971us/step - loss: 0.7065 - accuracy: 0.6605\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - 0s 774us/step - loss: 0.6399 - accuracy: 0.6692\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - 0s 709us/step - loss: 0.5970 - accuracy: 0.6960\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - 0s 748us/step - loss: 0.6466 - accuracy: 0.6476\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - 0s 696us/step - loss: 0.6576 - accuracy: 0.64470s - loss: 0.6577 - accuracy: 0.64\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - 0s 696us/step - loss: 0.6366 - accuracy: 0.6664\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - 0s 827us/step - loss: 0.6329 - accuracy: 0.6611\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - 0s 656us/step - loss: 0.6215 - accuracy: 0.6653\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - 0s 639us/step - loss: 0.6156 - accuracy: 0.6991\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - 0s 617us/step - loss: 0.6217 - accuracy: 0.6519\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - 0s 640us/step - loss: 0.6109 - accuracy: 0.6872\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - 0s 722us/step - loss: 0.6079 - accuracy: 0.6891\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - 0s 656us/step - loss: 0.6023 - accuracy: 0.6891\n",
      "Epoch 21/100\n",
      "77/77 [==============================] - 0s 656us/step - loss: 0.6353 - accuracy: 0.6470\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - 0s 635us/step - loss: 0.5862 - accuracy: 0.7003\n",
      "Epoch 23/100\n",
      "77/77 [==============================] - 0s 630us/step - loss: 0.6219 - accuracy: 0.6801\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - 0s 696us/step - loss: 0.6119 - accuracy: 0.6843\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - 0s 695us/step - loss: 0.6404 - accuracy: 0.6545\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - 0s 643us/step - loss: 0.6093 - accuracy: 0.6872\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - 0s 709us/step - loss: 0.5774 - accuracy: 0.7002\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - 0s 761us/step - loss: 0.5982 - accuracy: 0.7109\n",
      "Epoch 29/100\n",
      "77/77 [==============================] - 0s 840us/step - loss: 0.6417 - accuracy: 0.6632\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - 0s 748us/step - loss: 0.6026 - accuracy: 0.6749\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - 0s 656us/step - loss: 0.6100 - accuracy: 0.6507\n",
      "Epoch 32/100\n",
      "77/77 [==============================] - 0s 643us/step - loss: 0.6013 - accuracy: 0.7027\n",
      "Epoch 33/100\n",
      "77/77 [==============================] - 0s 735us/step - loss: 0.6094 - accuracy: 0.6877\n",
      "Epoch 34/100\n",
      "77/77 [==============================] - 0s 722us/step - loss: 0.5849 - accuracy: 0.6791\n",
      "Epoch 35/100\n",
      "77/77 [==============================] - 0s 905us/step - loss: 0.5712 - accuracy: 0.7284\n",
      "Epoch 36/100\n",
      "77/77 [==============================] - 0s 958us/step - loss: 0.6119 - accuracy: 0.6633\n",
      "Epoch 37/100\n",
      "77/77 [==============================] - 0s 800us/step - loss: 0.6002 - accuracy: 0.6957\n",
      "Epoch 38/100\n",
      "77/77 [==============================] - 0s 722us/step - loss: 0.5788 - accuracy: 0.6940\n",
      "Epoch 39/100\n",
      "77/77 [==============================] - 0s 722us/step - loss: 0.6252 - accuracy: 0.6702\n",
      "Epoch 40/100\n",
      "77/77 [==============================] - 0s 879us/step - loss: 0.5986 - accuracy: 0.6919\n",
      "Epoch 41/100\n",
      "77/77 [==============================] - 0s 748us/step - loss: 0.6245 - accuracy: 0.6510\n",
      "Epoch 42/100\n",
      "77/77 [==============================] - 0s 758us/step - loss: 0.6034 - accuracy: 0.6806\n",
      "Epoch 43/100\n",
      "77/77 [==============================] - 0s 821us/step - loss: 0.6071 - accuracy: 0.6709\n",
      "Epoch 44/100\n",
      "77/77 [==============================] - 0s 971us/step - loss: 0.6117 - accuracy: 0.6863\n",
      "Epoch 45/100\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6097 - accuracy: 0.6810\n",
      "Epoch 46/100\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6045 - accuracy: 0.6897\n",
      "Epoch 47/100\n",
      "77/77 [==============================] - 0s 873us/step - loss: 0.6103 - accuracy: 0.6950\n",
      "Epoch 48/100\n",
      "77/77 [==============================] - 0s 735us/step - loss: 0.5942 - accuracy: 0.6903\n",
      "Epoch 49/100\n",
      "77/77 [==============================] - 0s 682us/step - loss: 0.5891 - accuracy: 0.7067\n",
      "Epoch 50/100\n",
      "77/77 [==============================] - 0s 722us/step - loss: 0.5966 - accuracy: 0.6964\n",
      "Epoch 51/100\n",
      "77/77 [==============================] - 0s 643us/step - loss: 0.6036 - accuracy: 0.6820\n",
      "Epoch 52/100\n",
      "77/77 [==============================] - 0s 787us/step - loss: 0.5880 - accuracy: 0.7064\n",
      "Epoch 53/100\n",
      "77/77 [==============================] - 0s 866us/step - loss: 0.5902 - accuracy: 0.7042\n",
      "Epoch 54/100\n",
      "77/77 [==============================] - 0s 827us/step - loss: 0.5855 - accuracy: 0.6993\n",
      "Epoch 55/100\n",
      "77/77 [==============================] - 0s 840us/step - loss: 0.5818 - accuracy: 0.7149\n",
      "Epoch 56/100\n",
      "77/77 [==============================] - 0s 761us/step - loss: 0.5982 - accuracy: 0.6947\n",
      "Epoch 57/100\n",
      "77/77 [==============================] - 0s 630us/step - loss: 0.5727 - accuracy: 0.7148\n",
      "Epoch 58/100\n",
      "77/77 [==============================] - 0s 696us/step - loss: 0.5818 - accuracy: 0.6843\n",
      "Epoch 59/100\n",
      "77/77 [==============================] - 0s 682us/step - loss: 0.5914 - accuracy: 0.7021\n",
      "Epoch 60/100\n",
      "77/77 [==============================] - 0s 643us/step - loss: 0.5743 - accuracy: 0.7140\n",
      "Epoch 61/100\n",
      "77/77 [==============================] - 0s 630us/step - loss: 0.5828 - accuracy: 0.7212\n",
      "Epoch 62/100\n",
      "77/77 [==============================] - 0s 643us/step - loss: 0.5476 - accuracy: 0.7430\n",
      "Epoch 63/100\n",
      "77/77 [==============================] - 0s 722us/step - loss: 0.5401 - accuracy: 0.7187\n",
      "Epoch 64/100\n",
      "77/77 [==============================] - 0s 781us/step - loss: 0.5819 - accuracy: 0.7211\n",
      "Epoch 65/100\n",
      "77/77 [==============================] - 0s 997us/step - loss: 0.5653 - accuracy: 0.7007\n",
      "Epoch 66/100\n",
      "77/77 [==============================] - 0s 919us/step - loss: 0.5842 - accuracy: 0.7287\n",
      "Epoch 67/100\n",
      "77/77 [==============================] - 0s 691us/step - loss: 0.5728 - accuracy: 0.7093\n",
      "Epoch 68/100\n",
      "77/77 [==============================] - 0s 722us/step - loss: 0.5673 - accuracy: 0.7315\n",
      "Epoch 69/100\n",
      "77/77 [==============================] - 0s 643us/step - loss: 0.5906 - accuracy: 0.6716\n",
      "Epoch 70/100\n",
      "77/77 [==============================] - 0s 617us/step - loss: 0.5799 - accuracy: 0.6981\n",
      "Epoch 71/100\n",
      "77/77 [==============================] - 0s 628us/step - loss: 0.5722 - accuracy: 0.7066\n",
      "Epoch 72/100\n",
      "77/77 [==============================] - 0s 682us/step - loss: 0.5654 - accuracy: 0.7289\n",
      "Epoch 73/100\n",
      "77/77 [==============================] - 0s 690us/step - loss: 0.5755 - accuracy: 0.7104\n",
      "Epoch 74/100\n",
      "77/77 [==============================] - 0s 685us/step - loss: 0.5948 - accuracy: 0.6893\n",
      "Epoch 75/100\n",
      "77/77 [==============================] - 0s 748us/step - loss: 0.5469 - accuracy: 0.7380\n",
      "Epoch 76/100\n",
      "77/77 [==============================] - 0s 801us/step - loss: 0.5713 - accuracy: 0.7038\n",
      "Epoch 77/100\n",
      "77/77 [==============================] - 0s 814us/step - loss: 0.5561 - accuracy: 0.7452\n",
      "Epoch 78/100\n",
      "77/77 [==============================] - 0s 801us/step - loss: 0.5743 - accuracy: 0.7057\n",
      "Epoch 79/100\n",
      "77/77 [==============================] - 0s 866us/step - loss: 0.5655 - accuracy: 0.7286\n",
      "Epoch 80/100\n",
      "77/77 [==============================] - 0s 722us/step - loss: 0.5649 - accuracy: 0.7192\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 0s 761us/step - loss: 0.5779 - accuracy: 0.6961\n",
      "Epoch 82/100\n",
      "77/77 [==============================] - 0s 591us/step - loss: 0.5572 - accuracy: 0.7195\n",
      "Epoch 83/100\n",
      "77/77 [==============================] - 0s 604us/step - loss: 0.5713 - accuracy: 0.7393\n",
      "Epoch 84/100\n",
      "77/77 [==============================] - 0s 564us/step - loss: 0.5809 - accuracy: 0.6985\n",
      "Epoch 85/100\n",
      "77/77 [==============================] - 0s 591us/step - loss: 0.5588 - accuracy: 0.7167\n",
      "Epoch 86/100\n",
      "77/77 [==============================] - 0s 564us/step - loss: 0.5700 - accuracy: 0.7266\n",
      "Epoch 87/100\n",
      "77/77 [==============================] - 0s 577us/step - loss: 0.5798 - accuracy: 0.7149\n",
      "Epoch 88/100\n",
      "77/77 [==============================] - 0s 551us/step - loss: 0.5732 - accuracy: 0.7156\n",
      "Epoch 89/100\n",
      "77/77 [==============================] - 0s 591us/step - loss: 0.5659 - accuracy: 0.7127\n",
      "Epoch 90/100\n",
      "77/77 [==============================] - 0s 591us/step - loss: 0.6078 - accuracy: 0.7033\n",
      "Epoch 91/100\n",
      "77/77 [==============================] - 0s 591us/step - loss: 0.5658 - accuracy: 0.7175\n",
      "Epoch 92/100\n",
      "77/77 [==============================] - 0s 551us/step - loss: 0.5496 - accuracy: 0.7421\n",
      "Epoch 93/100\n",
      "77/77 [==============================] - 0s 559us/step - loss: 0.5557 - accuracy: 0.7311\n",
      "Epoch 94/100\n",
      "77/77 [==============================] - 0s 630us/step - loss: 0.5696 - accuracy: 0.7036\n",
      "Epoch 95/100\n",
      "77/77 [==============================] - 0s 800us/step - loss: 0.5526 - accuracy: 0.7260\n",
      "Epoch 96/100\n",
      "77/77 [==============================] - 0s 932us/step - loss: 0.5439 - accuracy: 0.7469\n",
      "Epoch 97/100\n",
      "77/77 [==============================] - 0s 827us/step - loss: 0.5771 - accuracy: 0.6847\n",
      "Epoch 98/100\n",
      "77/77 [==============================] - 0s 748us/step - loss: 0.5686 - accuracy: 0.7168\n",
      "Epoch 99/100\n",
      "77/77 [==============================] - 0s 591us/step - loss: 0.5609 - accuracy: 0.7180\n",
      "Epoch 100/100\n",
      "77/77 [==============================] - 0s 617us/step - loss: 0.5756 - accuracy: 0.6976\n",
      "Best: 0.688802 using {'epochs': 100, 'learning_rate': 0.001, 'momentum': 0.4}\n",
      "Mean: 0.589844, Std: 0.078967 with: {'epochs': 10, 'learning_rate': 0.001, 'momentum': 0.0}\n",
      "Mean: 0.631510, Std: 0.028940 with: {'epochs': 10, 'learning_rate': 0.001, 'momentum': 0.2}\n",
      "Mean: 0.657552, Std: 0.020256 with: {'epochs': 10, 'learning_rate': 0.001, 'momentum': 0.4}\n",
      "Mean: 0.640625, Std: 0.008438 with: {'epochs': 10, 'learning_rate': 0.001, 'momentum': 0.6}\n",
      "Mean: 0.666667, Std: 0.028940 with: {'epochs': 10, 'learning_rate': 0.001, 'momentum': 0.8}\n",
      "Mean: 0.665365, Std: 0.048405 with: {'epochs': 10, 'learning_rate': 0.001, 'momentum': 0.9}\n",
      "Mean: 0.639323, Std: 0.009207 with: {'epochs': 10, 'learning_rate': 0.01, 'momentum': 0.0}\n",
      "Mean: 0.644531, Std: 0.017758 with: {'epochs': 10, 'learning_rate': 0.01, 'momentum': 0.2}\n",
      "Mean: 0.647135, Std: 0.030145 with: {'epochs': 10, 'learning_rate': 0.01, 'momentum': 0.4}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 10, 'learning_rate': 0.01, 'momentum': 0.6}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 10, 'learning_rate': 0.01, 'momentum': 0.8}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 10, 'learning_rate': 0.01, 'momentum': 0.9}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 10, 'learning_rate': 0.1, 'momentum': 0.0}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 10, 'learning_rate': 0.1, 'momentum': 0.2}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 10, 'learning_rate': 0.1, 'momentum': 0.4}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 10, 'learning_rate': 0.1, 'momentum': 0.6}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 10, 'learning_rate': 0.1, 'momentum': 0.8}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 10, 'learning_rate': 0.1, 'momentum': 0.9}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 10, 'learning_rate': 0.2, 'momentum': 0.0}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 10, 'learning_rate': 0.2, 'momentum': 0.2}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 10, 'learning_rate': 0.2, 'momentum': 0.4}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 10, 'learning_rate': 0.2, 'momentum': 0.6}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 10, 'learning_rate': 0.2, 'momentum': 0.8}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 10, 'learning_rate': 0.2, 'momentum': 0.9}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 10, 'learning_rate': 0.3, 'momentum': 0.0}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 10, 'learning_rate': 0.3, 'momentum': 0.2}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 10, 'learning_rate': 0.3, 'momentum': 0.4}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 10, 'learning_rate': 0.3, 'momentum': 0.6}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 10, 'learning_rate': 0.3, 'momentum': 0.8}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 10, 'learning_rate': 0.3, 'momentum': 0.9}\n",
      "Mean: 0.641927, Std: 0.042830 with: {'epochs': 50, 'learning_rate': 0.001, 'momentum': 0.0}\n",
      "Mean: 0.651042, Std: 0.032106 with: {'epochs': 50, 'learning_rate': 0.001, 'momentum': 0.2}\n",
      "Mean: 0.661458, Std: 0.033197 with: {'epochs': 50, 'learning_rate': 0.001, 'momentum': 0.4}\n",
      "Mean: 0.674479, Std: 0.009744 with: {'epochs': 50, 'learning_rate': 0.001, 'momentum': 0.6}\n",
      "Mean: 0.670573, Std: 0.036966 with: {'epochs': 50, 'learning_rate': 0.001, 'momentum': 0.8}\n",
      "Mean: 0.658854, Std: 0.036690 with: {'epochs': 50, 'learning_rate': 0.001, 'momentum': 0.9}\n",
      "Mean: 0.648438, Std: 0.009568 with: {'epochs': 50, 'learning_rate': 0.01, 'momentum': 0.0}\n",
      "Mean: 0.664062, Std: 0.033299 with: {'epochs': 50, 'learning_rate': 0.01, 'momentum': 0.2}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 50, 'learning_rate': 0.01, 'momentum': 0.4}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 50, 'learning_rate': 0.01, 'momentum': 0.6}\n",
      "Mean: 0.649740, Std: 0.026557 with: {'epochs': 50, 'learning_rate': 0.01, 'momentum': 0.8}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 50, 'learning_rate': 0.01, 'momentum': 0.9}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 50, 'learning_rate': 0.1, 'momentum': 0.0}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 50, 'learning_rate': 0.1, 'momentum': 0.2}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 50, 'learning_rate': 0.1, 'momentum': 0.4}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 50, 'learning_rate': 0.1, 'momentum': 0.6}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 50, 'learning_rate': 0.1, 'momentum': 0.8}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 50, 'learning_rate': 0.1, 'momentum': 0.9}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 50, 'learning_rate': 0.2, 'momentum': 0.0}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 50, 'learning_rate': 0.2, 'momentum': 0.2}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 50, 'learning_rate': 0.2, 'momentum': 0.4}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 50, 'learning_rate': 0.2, 'momentum': 0.6}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 50, 'learning_rate': 0.2, 'momentum': 0.8}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 50, 'learning_rate': 0.2, 'momentum': 0.9}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 50, 'learning_rate': 0.3, 'momentum': 0.0}\n",
      "Mean: 0.649740, Std: 0.024360 with: {'epochs': 50, 'learning_rate': 0.3, 'momentum': 0.2}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 50, 'learning_rate': 0.3, 'momentum': 0.4}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 50, 'learning_rate': 0.3, 'momentum': 0.6}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 50, 'learning_rate': 0.3, 'momentum': 0.8}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 50, 'learning_rate': 0.3, 'momentum': 0.9}\n",
      "Mean: 0.657552, Std: 0.016053 with: {'epochs': 100, 'learning_rate': 0.001, 'momentum': 0.0}\n",
      "Mean: 0.686198, Std: 0.020256 with: {'epochs': 100, 'learning_rate': 0.001, 'momentum': 0.2}\n",
      "Mean: 0.688802, Std: 0.004872 with: {'epochs': 100, 'learning_rate': 0.001, 'momentum': 0.4}\n",
      "Mean: 0.679688, Std: 0.006379 with: {'epochs': 100, 'learning_rate': 0.001, 'momentum': 0.6}\n",
      "Mean: 0.651042, Std: 0.021710 with: {'epochs': 100, 'learning_rate': 0.001, 'momentum': 0.8}\n",
      "Mean: 0.644531, Std: 0.022097 with: {'epochs': 100, 'learning_rate': 0.001, 'momentum': 0.9}\n",
      "Mean: 0.640625, Std: 0.041463 with: {'epochs': 100, 'learning_rate': 0.01, 'momentum': 0.0}\n",
      "Mean: 0.654948, Std: 0.030314 with: {'epochs': 100, 'learning_rate': 0.01, 'momentum': 0.2}\n",
      "Mean: 0.651042, Std: 0.030647 with: {'epochs': 100, 'learning_rate': 0.01, 'momentum': 0.4}\n",
      "Mean: 0.649740, Std: 0.026557 with: {'epochs': 100, 'learning_rate': 0.01, 'momentum': 0.6}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 100, 'learning_rate': 0.01, 'momentum': 0.8}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 100, 'learning_rate': 0.01, 'momentum': 0.9}\n",
      "Mean: 0.649740, Std: 0.026557 with: {'epochs': 100, 'learning_rate': 0.1, 'momentum': 0.0}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 100, 'learning_rate': 0.1, 'momentum': 0.2}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 100, 'learning_rate': 0.1, 'momentum': 0.4}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 100, 'learning_rate': 0.1, 'momentum': 0.6}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 100, 'learning_rate': 0.1, 'momentum': 0.8}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 100, 'learning_rate': 0.1, 'momentum': 0.9}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 100, 'learning_rate': 0.2, 'momentum': 0.0}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 100, 'learning_rate': 0.2, 'momentum': 0.2}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 100, 'learning_rate': 0.2, 'momentum': 0.4}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 100, 'learning_rate': 0.2, 'momentum': 0.6}\n",
      "Mean: 0.652344, Std: 0.022999 with: {'epochs': 100, 'learning_rate': 0.2, 'momentum': 0.8}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 100, 'learning_rate': 0.2, 'momentum': 0.9}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 100, 'learning_rate': 0.3, 'momentum': 0.0}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 100, 'learning_rate': 0.3, 'momentum': 0.2}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 100, 'learning_rate': 0.3, 'momentum': 0.4}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 100, 'learning_rate': 0.3, 'momentum': 0.6}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 100, 'learning_rate': 0.3, 'momentum': 0.8}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'epochs': 100, 'learning_rate': 0.3, 'momentum': 0.9}\n"
     ]
    }
   ],
   "source": [
    "#It will be conducted using SGD optimizer as its hyperparameter is easy to understood\n",
    "from keras.optimizers import SGD\n",
    "def create_model(learning_rate=0.01, momentum=0):\n",
    "    #Create model\n",
    "    model = Sequential()\n",
    "    #Hidden layer\n",
    "    model.add(Dense(12,input_dim=X.shape[1], activation='relu'))\n",
    "    #Output Layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    #Compile Model\n",
    "    #Define optimizer\n",
    "    optimizer = SGD(lr=learning_rate, momentum=momentum)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#Fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "#Create model\n",
    "model = KerasClassifier(build_fn = create_model, batch_size=10, verbose=1)\n",
    "\n",
    "#Define grid search parameters\n",
    "learning_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "#I also optimize to the number of epochs, as it is dependent to the learning rate\n",
    "epochs = [10, 50, 100]\n",
    "param_grid = dict(learning_rate = learning_rate, momentum = momentum, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X,y)\n",
    "\n",
    "#Summarize results\n",
    "print(\"Best: %f using %s\" %(grid_result.best_score_, grid_result.best_params_))\n",
    "#Result at every parameter combination\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"Mean: %f, Std: %f with: %r\" %(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It is found that learning rate 0.001 and momentum 0.4 with 100 number of epochs is the optimal configuration. However, it\n",
    "#can also be observed that the performance is still worse than Adam optimization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.726562 using {'init_mode': 'uniform'}\n",
      "Mean: 0.726562, Std: 0.012758 with: {'init_mode': 'uniform'}\n",
      "Mean: 0.700521, Std: 0.024360 with: {'init_mode': 'lecun_uniform'}\n",
      "Mean: 0.714844, Std: 0.013902 with: {'init_mode': 'normal'}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'init_mode': 'zero'}\n",
      "Mean: 0.694010, Std: 0.011201 with: {'init_mode': 'glorot_normal'}\n",
      "Mean: 0.692708, Std: 0.018688 with: {'init_mode': 'glorot_uniform'}\n",
      "Mean: 0.648438, Std: 0.039192 with: {'init_mode': 'he_normal'}\n",
      "Mean: 0.687500, Std: 0.022097 with: {'init_mode': 'he_uniform'}\n"
     ]
    }
   ],
   "source": [
    "##Tune Network Weight Initialization\n",
    "def create_model(init_mode='uniform'):\n",
    "    #Create model\n",
    "    model = Sequential()\n",
    "    #Hidden layer\n",
    "    model.add(Dense(12,input_dim=X.shape[1], kernel_initializer=init_mode, activation='relu'))\n",
    "    #Output Layer\n",
    "    model.add(Dense(1, kernel_initializer=init_mode, activation='sigmoid'))\n",
    "    #Compile Model\n",
    "    #Define optimizer\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#Fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "#Create model\n",
    "model = KerasClassifier(build_fn = create_model, epochs = 100, batch_size=10, verbose=0)\n",
    "\n",
    "#Define grid search parameters\n",
    "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "param_grid = dict(init_mode=init_mode)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X,y)\n",
    "\n",
    "#Summarize results\n",
    "print(\"Best: %f using %s\" %(grid_result.best_score_, grid_result.best_params_))\n",
    "#Result at every parameter combination\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"Mean: %f, Std: %f with: %r\" %(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It is found that uniform weight initialization scheme is the best scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Tune Neuron Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.729167 using {'activation': 'softplus'}\n",
      "Mean: 0.721354, Std: 0.021710 with: {'activation': 'relu'}\n",
      "Mean: 0.639323, Std: 0.017566 with: {'activation': 'softmax'}\n",
      "Mean: 0.729167, Std: 0.009744 with: {'activation': 'softplus'}\n",
      "Mean: 0.690104, Std: 0.014382 with: {'activation': 'softsign'}\n",
      "Mean: 0.680990, Std: 0.040133 with: {'activation': 'tanh'}\n",
      "Mean: 0.699219, Std: 0.013902 with: {'activation': 'sigmoid'}\n",
      "Mean: 0.691406, Std: 0.013902 with: {'activation': 'hard_sigmoid'}\n",
      "Mean: 0.707031, Std: 0.022326 with: {'activation': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "def create_model(activation='relu'):\n",
    "    #Create model\n",
    "    model = Sequential()\n",
    "    #Hidden layer\n",
    "    model.add(Dense(12,input_dim=X.shape[1], kernel_initializer='uniform', activation=activation))\n",
    "    #Output Layer\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "    #Compile Model\n",
    "    #Define optimizer\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#Fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "#Create model\n",
    "model = KerasClassifier(build_fn = create_model, epochs = 100, batch_size=10, verbose=0)\n",
    "\n",
    "#Define grid search parameters\n",
    "activation = ['relu', 'softmax', 'softplus', 'softsign', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "param_grid = dict(activation=activation)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X,y)\n",
    "\n",
    "#Summarize results\n",
    "print(\"Best: %f using %s\" %(grid_result.best_score_, grid_result.best_params_))\n",
    "#Result at every parameter combination\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"Mean: %f, Std: %f with: %r\" %(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It is found softplus to be the optimal activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Tune dropout Regulatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.738281 using {'dropout_rate': 0.1, 'weight_constraint': 1}\n",
      "Mean: 0.726562, Std: 0.030425 with: {'dropout_rate': 0.0, 'weight_constraint': 1}\n",
      "Mean: 0.710938, Std: 0.013902 with: {'dropout_rate': 0.0, 'weight_constraint': 2}\n",
      "Mean: 0.723958, Std: 0.009207 with: {'dropout_rate': 0.0, 'weight_constraint': 3}\n",
      "Mean: 0.734375, Std: 0.005524 with: {'dropout_rate': 0.0, 'weight_constraint': 4}\n",
      "Mean: 0.730469, Std: 0.027805 with: {'dropout_rate': 0.0, 'weight_constraint': 5}\n",
      "Mean: 0.738281, Std: 0.019401 with: {'dropout_rate': 0.1, 'weight_constraint': 1}\n",
      "Mean: 0.716146, Std: 0.003683 with: {'dropout_rate': 0.1, 'weight_constraint': 2}\n",
      "Mean: 0.705729, Std: 0.012890 with: {'dropout_rate': 0.1, 'weight_constraint': 3}\n",
      "Mean: 0.730469, Std: 0.024910 with: {'dropout_rate': 0.1, 'weight_constraint': 4}\n",
      "Mean: 0.727865, Std: 0.004872 with: {'dropout_rate': 0.1, 'weight_constraint': 5}\n",
      "Mean: 0.713542, Std: 0.026557 with: {'dropout_rate': 0.2, 'weight_constraint': 1}\n",
      "Mean: 0.717448, Std: 0.028764 with: {'dropout_rate': 0.2, 'weight_constraint': 2}\n",
      "Mean: 0.714844, Std: 0.015947 with: {'dropout_rate': 0.2, 'weight_constraint': 3}\n",
      "Mean: 0.730469, Std: 0.026107 with: {'dropout_rate': 0.2, 'weight_constraint': 4}\n",
      "Mean: 0.730469, Std: 0.019918 with: {'dropout_rate': 0.2, 'weight_constraint': 5}\n",
      "Mean: 0.723958, Std: 0.022628 with: {'dropout_rate': 0.3, 'weight_constraint': 1}\n",
      "Mean: 0.709635, Std: 0.009207 with: {'dropout_rate': 0.3, 'weight_constraint': 2}\n",
      "Mean: 0.723958, Std: 0.006639 with: {'dropout_rate': 0.3, 'weight_constraint': 3}\n",
      "Mean: 0.721354, Std: 0.013279 with: {'dropout_rate': 0.3, 'weight_constraint': 4}\n",
      "Mean: 0.723958, Std: 0.027498 with: {'dropout_rate': 0.3, 'weight_constraint': 5}\n",
      "Mean: 0.722656, Std: 0.020915 with: {'dropout_rate': 0.4, 'weight_constraint': 1}\n",
      "Mean: 0.721354, Std: 0.018414 with: {'dropout_rate': 0.4, 'weight_constraint': 2}\n",
      "Mean: 0.701823, Std: 0.012075 with: {'dropout_rate': 0.4, 'weight_constraint': 3}\n",
      "Mean: 0.704427, Std: 0.018688 with: {'dropout_rate': 0.4, 'weight_constraint': 4}\n",
      "Mean: 0.714844, Std: 0.015947 with: {'dropout_rate': 0.4, 'weight_constraint': 5}\n",
      "Mean: 0.700521, Std: 0.038976 with: {'dropout_rate': 0.5, 'weight_constraint': 1}\n",
      "Mean: 0.708333, Std: 0.020505 with: {'dropout_rate': 0.5, 'weight_constraint': 2}\n",
      "Mean: 0.686198, Std: 0.019488 with: {'dropout_rate': 0.5, 'weight_constraint': 3}\n",
      "Mean: 0.703125, Std: 0.006379 with: {'dropout_rate': 0.5, 'weight_constraint': 4}\n",
      "Mean: 0.704427, Std: 0.012075 with: {'dropout_rate': 0.5, 'weight_constraint': 5}\n",
      "Mean: 0.703125, Std: 0.013902 with: {'dropout_rate': 0.6, 'weight_constraint': 1}\n",
      "Mean: 0.688802, Std: 0.019225 with: {'dropout_rate': 0.6, 'weight_constraint': 2}\n",
      "Mean: 0.683594, Std: 0.027805 with: {'dropout_rate': 0.6, 'weight_constraint': 3}\n",
      "Mean: 0.703125, Std: 0.032369 with: {'dropout_rate': 0.6, 'weight_constraint': 4}\n",
      "Mean: 0.699219, Std: 0.011500 with: {'dropout_rate': 0.6, 'weight_constraint': 5}\n",
      "Mean: 0.699219, Std: 0.019137 with: {'dropout_rate': 0.7, 'weight_constraint': 1}\n",
      "Mean: 0.686198, Std: 0.014382 with: {'dropout_rate': 0.7, 'weight_constraint': 2}\n",
      "Mean: 0.667969, Std: 0.026107 with: {'dropout_rate': 0.7, 'weight_constraint': 3}\n",
      "Mean: 0.665365, Std: 0.022628 with: {'dropout_rate': 0.7, 'weight_constraint': 4}\n",
      "Mean: 0.667969, Std: 0.022326 with: {'dropout_rate': 0.7, 'weight_constraint': 5}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'dropout_rate': 0.8, 'weight_constraint': 1}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'dropout_rate': 0.8, 'weight_constraint': 2}\n",
      "Mean: 0.653646, Std: 0.027498 with: {'dropout_rate': 0.8, 'weight_constraint': 3}\n",
      "Mean: 0.657552, Std: 0.019225 with: {'dropout_rate': 0.8, 'weight_constraint': 4}\n",
      "Mean: 0.653646, Std: 0.021236 with: {'dropout_rate': 0.8, 'weight_constraint': 5}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'dropout_rate': 0.9, 'weight_constraint': 1}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'dropout_rate': 0.9, 'weight_constraint': 2}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'dropout_rate': 0.9, 'weight_constraint': 3}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'dropout_rate': 0.9, 'weight_constraint': 4}\n",
      "Mean: 0.651042, Std: 0.024774 with: {'dropout_rate': 0.9, 'weight_constraint': 5}\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "def create_model(dropout_rate = 0.0, weight_constraint = 0):\n",
    "    #Create model\n",
    "    model = Sequential()\n",
    "    #Hidden layer\n",
    "    model.add(Dense(12,input_dim=X.shape[1], kernel_initializer='uniform', activation='softplus', kernel_constraint=maxnorm(weight_constraint)))\n",
    "    #Dropout\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    #Output Layer\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "    #Compile Model\n",
    "    #Define optimizer\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#Fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "#Create model\n",
    "model = KerasClassifier(build_fn = create_model, epochs = 100, batch_size=10, verbose=0)\n",
    "\n",
    "#Define grid search parameters\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "weight_constraint = [1, 2, 3, 4, 5]\n",
    "param_grid = dict(dropout_rate = dropout_rate, weight_constraint = weight_constraint)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X,y)\n",
    "\n",
    "#Summarize results\n",
    "print(\"Best: %f using %s\" %(grid_result.best_score_, grid_result.best_params_))\n",
    "#Result at every parameter combination\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"Mean: %f, Std: %f with: %r\" %(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It is found that dropout rate 0.1 with weight constraint = 1 is the optimal dropout configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Tune number of neurons in the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.738281 using {'n_neuron': 30}\n",
      "Mean: 0.680990, Std: 0.062364 with: {'n_neuron': 1}\n",
      "Mean: 0.690104, Std: 0.033502 with: {'n_neuron': 5}\n",
      "Mean: 0.725260, Std: 0.023939 with: {'n_neuron': 10}\n",
      "Mean: 0.716146, Std: 0.004872 with: {'n_neuron': 12}\n",
      "Mean: 0.731771, Std: 0.022402 with: {'n_neuron': 15}\n",
      "Mean: 0.735677, Std: 0.006639 with: {'n_neuron': 20}\n",
      "Mean: 0.735677, Std: 0.025780 with: {'n_neuron': 25}\n",
      "Mean: 0.738281, Std: 0.020915 with: {'n_neuron': 30}\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "def create_model(n_neuron= 12):\n",
    "    #Create model\n",
    "    model = Sequential()\n",
    "    #Hidden layer\n",
    "    model.add(Dense(n_neuron,input_dim=X.shape[1], kernel_initializer='uniform', activation='softplus', kernel_constraint=maxnorm(1)))\n",
    "    #Dropout\n",
    "    model.add(Dropout(0.1))\n",
    "    #Output Layer\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "    #Compile Model\n",
    "    #Define optimizer\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#Fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "#Create model\n",
    "model = KerasClassifier(build_fn = create_model, epochs = 100, batch_size=10, verbose=0)\n",
    "\n",
    "#Define grid search parameters\n",
    "n_neuron = [1, 5, 10, 12, 15, 20, 25, 30]\n",
    "param_grid = dict(n_neuron = n_neuron)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X,y)\n",
    "\n",
    "#Summarize results\n",
    "print(\"Best: %f using %s\" %(grid_result.best_score_, grid_result.best_params_))\n",
    "#Result at every parameter combination\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"Mean: %f, Std: %f with: %r\" %(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It is found 30 number of neurons is the optimal found. Note that 30 is the maximum boundary of the defined grid parameter,\n",
    "#therefore, it is suggested to enlarge the range of the parameters to discover the potential of a better solution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
